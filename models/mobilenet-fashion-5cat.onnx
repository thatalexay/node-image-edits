
pytorch2.9.1:Ì¯
›
input
features.0.0.weight
features.0.0.weight_biasgetitemnode_Conv_705"Conv*
group†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J≠
	namespaceü: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.0: torchvision.ops.misc.Conv2dNormActivation/features.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJÜ
pkg.torch.onnx.class_hierarchy„['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J∏
pkg.torch.onnx.fx_nodeù%_native_batch_norm_legit_no_training : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d, %p_features_0_1_weight, %p_features_0_1_bias, %b_features_0_1_running_mean, %b_features_0_1_running_var, 0.1, 1e-05), kwargs = {})Jt
pkg.torch.onnx.name_scopesV['', 'features', 'features.0', 'features.0.1', '_native_batch_norm_legit_no_training']JÊ
pkg.torch.onnx.stack_trace«File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
û
getitem
min_val_cast
max_val_casthardtanhn4"ClipJ
	namespace‚: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.0: torchvision.ops.misc.Conv2dNormActivation/features.0.2: torch.nn.modules.activation.ReLU6/hardtanh: aten.hardtanh.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jï
pkg.torch.onnx.fx_node{%hardtanh : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem, 0.0, 6.0), kwargs = {})JX
pkg.torch.onnx.name_scopes:['', 'features', 'features.0', 'features.0.2', 'hardtanh']Jï
pkg.torch.onnx.stack_traceˆFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
É
hardtanh
features.1.conv.0.0.weight
features.1.conv.0.0.weight_bias	getitem_3node_Conv_707"Conv*
group †*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.1: torchvision.models.mobilenetv2.InvertedResidual/features.1.conv: torch.nn.modules.container.Sequential/features.1.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.1.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_1: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_1 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_1, %p_features_1_conv_0_1_weight, %p_features_1_conv_0_1_bias, %b_features_1_conv_0_1_running_mean, %b_features_1_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.1', 'features.1.conv', 'features.1.conv.0', 'features.1.conv.0.1', '_native_batch_norm_legit_no_training_1']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
ö
	getitem_3
min_val_cast
max_val_cast
hardtanh_1n4_2"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.1: torchvision.models.mobilenetv2.InvertedResidual/features.1.conv: torch.nn.modules.container.Sequential/features.1.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.1.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_1: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jô
pkg.torch.onnx.fx_node%hardtanh_1 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_3, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.1', 'features.1.conv', 'features.1.conv.0', 'features.1.conv.0.2', 'hardtanh_1']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
≈

hardtanh_1
features.1.conv.1.weight
features.1.conv.1.weight_bias	getitem_6node_Conv_709"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†JÒ
	namespace„: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.1: torchvision.models.mobilenetv2.InvertedResidual/features.1.conv: torch.nn.modules.container.Sequential/features.1.conv.2: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_2: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J–
pkg.torch.onnx.fx_nodeµ%_native_batch_norm_legit_no_training_2 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_2, %p_features_1_conv_2_weight, %p_features_1_conv_2_bias, %b_features_1_conv_2_running_mean, %b_features_1_conv_2_running_var, 0.1, 1e-05), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.1', 'features.1.conv', 'features.1.conv.2', '_native_batch_norm_legit_no_training_2']Jó
pkg.torch.onnx.stack_trace¯File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
Ñ
	getitem_6
features.2.conv.0.0.weight
features.2.conv.0.0.weight_bias	getitem_9node_Conv_711"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.2: torchvision.models.mobilenetv2.InvertedResidual/features.2.conv: torch.nn.modules.container.Sequential/features.2.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.2.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_3: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_3 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_3, %p_features_2_conv_0_1_weight, %p_features_2_conv_0_1_bias, %b_features_2_conv_0_1_running_mean, %b_features_2_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.2', 'features.2.conv', 'features.2.conv.0', 'features.2.conv.0.1', '_native_batch_norm_legit_no_training_3']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
ö
	getitem_9
min_val_cast
max_val_cast
hardtanh_2n4_3"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.2: torchvision.models.mobilenetv2.InvertedResidual/features.2.conv: torch.nn.modules.container.Sequential/features.2.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.2.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_2: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jô
pkg.torch.onnx.fx_node%hardtanh_2 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_9, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.2', 'features.2.conv', 'features.2.conv.0', 'features.2.conv.0.2', 'hardtanh_2']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
Ü

hardtanh_2
features.2.conv.1.0.weight
features.2.conv.1.0.weight_bias
getitem_12node_Conv_713"Conv*
group`†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.2: torchvision.models.mobilenetv2.InvertedResidual/features.2.conv: torch.nn.modules.container.Sequential/features.2.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.2.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_4: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_4 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_4, %p_features_2_conv_1_1_weight, %p_features_2_conv_1_1_bias, %b_features_2_conv_1_1_running_mean, %b_features_2_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.2', 'features.2.conv', 'features.2.conv.1', 'features.2.conv.1.1', '_native_batch_norm_legit_no_training_4']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
ù

getitem_12
min_val_cast
max_val_cast
hardtanh_3n4_4"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.2: torchvision.models.mobilenetv2.InvertedResidual/features.2.conv: torch.nn.modules.container.Sequential/features.2.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.2.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_3: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_3 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_12, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.2', 'features.2.conv', 'features.2.conv.1', 'features.2.conv.1.2', 'hardtanh_3']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
∆

hardtanh_3
features.2.conv.2.weight
features.2.conv.2.weight_bias
getitem_15node_Conv_715"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†JÒ
	namespace„: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.2: torchvision.models.mobilenetv2.InvertedResidual/features.2.conv: torch.nn.modules.container.Sequential/features.2.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_5: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J–
pkg.torch.onnx.fx_nodeµ%_native_batch_norm_legit_no_training_5 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_5, %p_features_2_conv_3_weight, %p_features_2_conv_3_bias, %b_features_2_conv_3_running_mean, %b_features_2_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.2', 'features.2.conv', 'features.2.conv.3', '_native_batch_norm_legit_no_training_5']Jó
pkg.torch.onnx.stack_trace¯File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
ä

getitem_15
features.3.conv.0.0.weight
features.3.conv.0.0.weight_bias
getitem_18node_Conv_717"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/features.3.conv: torch.nn.modules.container.Sequential/features.3.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.3.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_6: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_6 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_6, %p_features_3_conv_0_1_weight, %p_features_3_conv_0_1_bias, %b_features_3_conv_0_1_running_mean, %b_features_3_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.3', 'features.3.conv', 'features.3.conv.0', 'features.3.conv.0.1', '_native_batch_norm_legit_no_training_6']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
°

getitem_18
min_val_cast
max_val_cast
hardtanh_4n4_5"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/features.3.conv: torch.nn.modules.container.Sequential/features.3.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.3.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_4: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_4 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_18, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.3', 'features.3.conv', 'features.3.conv.0', 'features.3.conv.0.2', 'hardtanh_4']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ã

hardtanh_4
features.3.conv.1.0.weight
features.3.conv.1.0.weight_bias
getitem_21node_Conv_719"Conv*
groupê†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/features.3.conv: torch.nn.modules.container.Sequential/features.3.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.3.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_7: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_7 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_7, %p_features_3_conv_1_1_weight, %p_features_3_conv_1_1_bias, %b_features_3_conv_1_1_running_mean, %b_features_3_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.3', 'features.3.conv', 'features.3.conv.1', 'features.3.conv.1.1', '_native_batch_norm_legit_no_training_7']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
°

getitem_21
min_val_cast
max_val_cast
hardtanh_5n4_6"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/features.3.conv: torch.nn.modules.container.Sequential/features.3.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.3.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_5: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_5 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_21, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.3', 'features.3.conv', 'features.3.conv.1', 'features.3.conv.1.2', 'hardtanh_5']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
 

hardtanh_5
features.3.conv.2.weight
features.3.conv.2.weight_bias
getitem_24node_Conv_721"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†JÒ
	namespace„: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/features.3.conv: torch.nn.modules.container.Sequential/features.3.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_8: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J–
pkg.torch.onnx.fx_nodeµ%_native_batch_norm_legit_no_training_8 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_8, %p_features_3_conv_3_weight, %p_features_3_conv_3_bias, %b_features_3_conv_3_running_mean, %b_features_3_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.3', 'features.3.conv', 'features.3.conv.3', '_native_batch_norm_legit_no_training_8']Jõ
pkg.torch.onnx.stack_trace¸File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
ƒ	

getitem_15

getitem_24addnode_add"AddJª
	namespace≠: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.3: torchvision.models.mobilenetv2.InvertedResidual/add: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jê
pkg.torch.onnx.fx_nodev%add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_15, %getitem_24), kwargs = {})JC
pkg.torch.onnx.name_scopes%['', 'features', 'features.3', 'add']Jº
pkg.torch.onnx.stack_traceùFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
ˇ
add
features.4.conv.0.0.weight
features.4.conv.0.0.weight_bias
getitem_27node_Conv_723"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J∞
	namespace¢: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.4: torchvision.models.mobilenetv2.InvertedResidual/features.4.conv: torch.nn.modules.container.Sequential/features.4.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.4.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_9: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jÿ
pkg.torch.onnx.fx_nodeΩ%_native_batch_norm_legit_no_training_9 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_9, %p_features_4_conv_0_1_weight, %p_features_4_conv_0_1_bias, %b_features_4_conv_0_1_running_mean, %b_features_4_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J¶
pkg.torch.onnx.name_scopesá['', 'features', 'features.4', 'features.4.conv', 'features.4.conv.0', 'features.4.conv.0.1', '_native_batch_norm_legit_no_training_9']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
ù

getitem_27
min_val_cast
max_val_cast
hardtanh_6n4_7"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.4: torchvision.models.mobilenetv2.InvertedResidual/features.4.conv: torch.nn.modules.container.Sequential/features.4.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.4.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_6: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_6 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_27, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.4', 'features.4.conv', 'features.4.conv.0', 'features.4.conv.0.2', 'hardtanh_6']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ã

hardtanh_6
features.4.conv.1.0.weight
features.4.conv.1.0.weight_bias
getitem_30node_Conv_725"Conv*
groupê†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.4: torchvision.models.mobilenetv2.InvertedResidual/features.4.conv: torch.nn.modules.container.Sequential/features.4.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.4.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_10: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_10 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_10, %p_features_4_conv_1_1_weight, %p_features_4_conv_1_1_bias, %b_features_4_conv_1_1_running_mean, %b_features_4_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.4', 'features.4.conv', 'features.4.conv.1', 'features.4.conv.1.1', '_native_batch_norm_legit_no_training_10']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
ù

getitem_30
min_val_cast
max_val_cast
hardtanh_7n4_8"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.4: torchvision.models.mobilenetv2.InvertedResidual/features.4.conv: torch.nn.modules.container.Sequential/features.4.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.4.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_7: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_7 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_30, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.4', 'features.4.conv', 'features.4.conv.1', 'features.4.conv.1.2', 'hardtanh_7']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
 

hardtanh_7
features.4.conv.2.weight
features.4.conv.2.weight_bias
getitem_33node_Conv_727"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.4: torchvision.models.mobilenetv2.InvertedResidual/features.4.conv: torch.nn.modules.container.Sequential/features.4.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_11: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_11 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_11, %p_features_4_conv_3_weight, %p_features_4_conv_3_bias, %b_features_4_conv_3_running_mean, %b_features_4_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.4', 'features.4.conv', 'features.4.conv.3', '_native_batch_norm_legit_no_training_11']Jó
pkg.torch.onnx.stack_trace¯File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
é

getitem_33
features.5.conv.0.0.weight
features.5.conv.0.0.weight_bias
getitem_36node_Conv_729"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/features.5.conv: torch.nn.modules.container.Sequential/features.5.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.5.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_12: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_12 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_12, %p_features_5_conv_0_1_weight, %p_features_5_conv_0_1_bias, %b_features_5_conv_0_1_running_mean, %b_features_5_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.5', 'features.5.conv', 'features.5.conv.0', 'features.5.conv.0.1', '_native_batch_norm_legit_no_training_12']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
°

getitem_36
min_val_cast
max_val_cast
hardtanh_8n4_9"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/features.5.conv: torch.nn.modules.container.Sequential/features.5.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.5.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_8: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_8 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_36, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.5', 'features.5.conv', 'features.5.conv.0', 'features.5.conv.0.2', 'hardtanh_8']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
è

hardtanh_8
features.5.conv.1.0.weight
features.5.conv.1.0.weight_bias
getitem_39node_Conv_731"Conv*
group¿†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/features.5.conv: torch.nn.modules.container.Sequential/features.5.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.5.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_13: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_13 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_13, %p_features_5_conv_1_1_weight, %p_features_5_conv_1_1_bias, %b_features_5_conv_1_1_running_mean, %b_features_5_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.5', 'features.5.conv', 'features.5.conv.1', 'features.5.conv.1.1', '_native_batch_norm_legit_no_training_13']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¢

getitem_39
min_val_cast
max_val_cast
hardtanh_9n4_10"ClipJÛ
	namespaceÂ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/features.5.conv: torch.nn.modules.container.Sequential/features.5.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.5.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_9: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jõ
pkg.torch.onnx.fx_nodeÄ%hardtanh_9 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_39, 0.0, 6.0), kwargs = {})Jâ
pkg.torch.onnx.name_scopesk['', 'features', 'features.5', 'features.5.conv', 'features.5.conv.1', 'features.5.conv.1.2', 'hardtanh_9']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
Œ

hardtanh_9
features.5.conv.2.weight
features.5.conv.2.weight_bias
getitem_42node_Conv_733"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/features.5.conv: torch.nn.modules.container.Sequential/features.5.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_14: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_14 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_14, %p_features_5_conv_3_weight, %p_features_5_conv_3_bias, %b_features_5_conv_3_running_mean, %b_features_5_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.5', 'features.5.conv', 'features.5.conv.3', '_native_batch_norm_legit_no_training_14']Jõ
pkg.torch.onnx.stack_trace¸File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
Œ	

getitem_33

getitem_42add_1
node_add_1"AddJΩ
	namespaceØ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.5: torchvision.models.mobilenetv2.InvertedResidual/add_1: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jí
pkg.torch.onnx.fx_nodex%add_1 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_33, %getitem_42), kwargs = {})JE
pkg.torch.onnx.name_scopes'['', 'features', 'features.5', 'add_1']Jº
pkg.torch.onnx.stack_traceùFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
â
add_1
features.6.conv.0.0.weight
features.6.conv.0.0.weight_bias
getitem_45node_Conv_735"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/features.6.conv: torch.nn.modules.container.Sequential/features.6.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.6.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_15: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_15 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_15, %p_features_6_conv_0_1_weight, %p_features_6_conv_0_1_bias, %b_features_6_conv_0_1_running_mean, %b_features_6_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.6', 'features.6.conv', 'features.6.conv.0', 'features.6.conv.0.1', '_native_batch_norm_legit_no_training_15']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_45
min_val_cast
max_val_casthardtanh_10n4_11"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/features.6.conv: torch.nn.modules.container.Sequential/features.6.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.6.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_10: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_10 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_45, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.6', 'features.6.conv', 'features.6.conv.0', 'features.6.conv.0.2', 'hardtanh_10']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ê
hardtanh_10
features.6.conv.1.0.weight
features.6.conv.1.0.weight_bias
getitem_48node_Conv_737"Conv*
group¿†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/features.6.conv: torch.nn.modules.container.Sequential/features.6.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.6.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_16: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_16 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_16, %p_features_6_conv_1_1_weight, %p_features_6_conv_1_1_bias, %b_features_6_conv_1_1_running_mean, %b_features_6_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.6', 'features.6.conv', 'features.6.conv.1', 'features.6.conv.1.1', '_native_batch_norm_legit_no_training_16']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_48
min_val_cast
max_val_casthardtanh_11n4_12"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/features.6.conv: torch.nn.modules.container.Sequential/features.6.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.6.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_11: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_11 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_48, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.6', 'features.6.conv', 'features.6.conv.1', 'features.6.conv.1.2', 'hardtanh_11']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
œ
hardtanh_11
features.6.conv.2.weight
features.6.conv.2.weight_bias
getitem_51node_Conv_739"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/features.6.conv: torch.nn.modules.container.Sequential/features.6.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_17: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_17 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_17, %p_features_6_conv_3_weight, %p_features_6_conv_3_bias, %b_features_6_conv_3_running_mean, %b_features_6_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.6', 'features.6.conv', 'features.6.conv.3', '_native_batch_norm_legit_no_training_17']Jõ
pkg.torch.onnx.stack_trace¸File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
ƒ	
add_1

getitem_51add_2
node_add_2"AddJΩ
	namespaceØ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.6: torchvision.models.mobilenetv2.InvertedResidual/add_2: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jç
pkg.torch.onnx.fx_nodes%add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1, %getitem_51), kwargs = {})JE
pkg.torch.onnx.name_scopes'['', 'features', 'features.6', 'add_2']Jº
pkg.torch.onnx.stack_traceùFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
Ö
add_2
features.7.conv.0.0.weight
features.7.conv.0.0.weight_bias
getitem_54node_Conv_741"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.7: torchvision.models.mobilenetv2.InvertedResidual/features.7.conv: torch.nn.modules.container.Sequential/features.7.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.7.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_18: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_18 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_18, %p_features_7_conv_0_1_weight, %p_features_7_conv_0_1_bias, %b_features_7_conv_0_1_running_mean, %b_features_7_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.7', 'features.7.conv', 'features.7.conv.0', 'features.7.conv.0.1', '_native_batch_norm_legit_no_training_18']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¢

getitem_54
min_val_cast
max_val_casthardtanh_12n4_13"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.7: torchvision.models.mobilenetv2.InvertedResidual/features.7.conv: torch.nn.modules.container.Sequential/features.7.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.7.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_12: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_12 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_54, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.7', 'features.7.conv', 'features.7.conv.0', 'features.7.conv.0.2', 'hardtanh_12']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
å
hardtanh_12
features.7.conv.1.0.weight
features.7.conv.1.0.weight_bias
getitem_57node_Conv_743"Conv*
group¿†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.7: torchvision.models.mobilenetv2.InvertedResidual/features.7.conv: torch.nn.modules.container.Sequential/features.7.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.7.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_19: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_19 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_19, %p_features_7_conv_1_1_weight, %p_features_7_conv_1_1_bias, %b_features_7_conv_1_1_running_mean, %b_features_7_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.7', 'features.7.conv', 'features.7.conv.1', 'features.7.conv.1.1', '_native_batch_norm_legit_no_training_19']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¢

getitem_57
min_val_cast
max_val_casthardtanh_13n4_14"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.7: torchvision.models.mobilenetv2.InvertedResidual/features.7.conv: torch.nn.modules.container.Sequential/features.7.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.7.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_13: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_13 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_57, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.7', 'features.7.conv', 'features.7.conv.1', 'features.7.conv.1.2', 'hardtanh_13']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
À
hardtanh_13
features.7.conv.2.weight
features.7.conv.2.weight_bias
getitem_60node_Conv_745"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.7: torchvision.models.mobilenetv2.InvertedResidual/features.7.conv: torch.nn.modules.container.Sequential/features.7.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_20: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_20 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_20, %p_features_7_conv_3_weight, %p_features_7_conv_3_bias, %b_features_7_conv_3_running_mean, %b_features_7_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.7', 'features.7.conv', 'features.7.conv.3', '_native_batch_norm_legit_no_training_20']Jó
pkg.torch.onnx.stack_trace¯File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
é

getitem_60
features.8.conv.0.0.weight
features.8.conv.0.0.weight_bias
getitem_63node_Conv_747"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/features.8.conv: torch.nn.modules.container.Sequential/features.8.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.8.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_21: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_21 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_21, %p_features_8_conv_0_1_weight, %p_features_8_conv_0_1_bias, %b_features_8_conv_0_1_running_mean, %b_features_8_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.8', 'features.8.conv', 'features.8.conv.0', 'features.8.conv.0.1', '_native_batch_norm_legit_no_training_21']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_63
min_val_cast
max_val_casthardtanh_14n4_15"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/features.8.conv: torch.nn.modules.container.Sequential/features.8.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.8.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_14: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_14 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_63, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.8', 'features.8.conv', 'features.8.conv.0', 'features.8.conv.0.2', 'hardtanh_14']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ê
hardtanh_14
features.8.conv.1.0.weight
features.8.conv.1.0.weight_bias
getitem_66node_Conv_749"Conv*
groupÄ†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/features.8.conv: torch.nn.modules.container.Sequential/features.8.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.8.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_22: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_22 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_22, %p_features_8_conv_1_1_weight, %p_features_8_conv_1_1_bias, %b_features_8_conv_1_1_running_mean, %b_features_8_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.8', 'features.8.conv', 'features.8.conv.1', 'features.8.conv.1.1', '_native_batch_norm_legit_no_training_22']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_66
min_val_cast
max_val_casthardtanh_15n4_16"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/features.8.conv: torch.nn.modules.container.Sequential/features.8.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.8.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_15: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_15 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_66, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.8', 'features.8.conv', 'features.8.conv.1', 'features.8.conv.1.2', 'hardtanh_15']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
œ
hardtanh_15
features.8.conv.2.weight
features.8.conv.2.weight_bias
getitem_69node_Conv_751"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/features.8.conv: torch.nn.modules.container.Sequential/features.8.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_23: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_23 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_23, %p_features_8_conv_3_weight, %p_features_8_conv_3_bias, %b_features_8_conv_3_running_mean, %b_features_8_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.8', 'features.8.conv', 'features.8.conv.3', '_native_batch_norm_legit_no_training_23']Jõ
pkg.torch.onnx.stack_trace¸File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
Œ	

getitem_60

getitem_69add_3
node_add_3"AddJΩ
	namespaceØ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.8: torchvision.models.mobilenetv2.InvertedResidual/add_3: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jí
pkg.torch.onnx.fx_nodex%add_3 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_60, %getitem_69), kwargs = {})JE
pkg.torch.onnx.name_scopes'['', 'features', 'features.8', 'add_3']Jº
pkg.torch.onnx.stack_traceùFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
â
add_3
features.9.conv.0.0.weight
features.9.conv.0.0.weight_bias
getitem_72node_Conv_753"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/features.9.conv: torch.nn.modules.container.Sequential/features.9.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.9.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_24: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_24 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_24, %p_features_9_conv_0_1_weight, %p_features_9_conv_0_1_bias, %b_features_9_conv_0_1_running_mean, %b_features_9_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.9', 'features.9.conv', 'features.9.conv.0', 'features.9.conv.0.1', '_native_batch_norm_legit_no_training_24']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_72
min_val_cast
max_val_casthardtanh_16n4_17"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/features.9.conv: torch.nn.modules.container.Sequential/features.9.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.9.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_16: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_16 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_72, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.9', 'features.9.conv', 'features.9.conv.0', 'features.9.conv.0.2', 'hardtanh_16']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ê
hardtanh_16
features.9.conv.1.0.weight
features.9.conv.1.0.weight_bias
getitem_75node_Conv_755"Conv*
groupÄ†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J±
	namespace£: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/features.9.conv: torch.nn.modules.container.Sequential/features.9.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.9.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_25: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J⁄
pkg.torch.onnx.fx_nodeø%_native_batch_norm_legit_no_training_25 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_25, %p_features_9_conv_1_1_weight, %p_features_9_conv_1_1_bias, %b_features_9_conv_1_1_running_mean, %b_features_9_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'features', 'features.9', 'features.9.conv', 'features.9.conv.1', 'features.9.conv.1.1', '_native_batch_norm_legit_no_training_25']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¶

getitem_75
min_val_cast
max_val_casthardtanh_17n4_18"ClipJÙ
	namespaceÊ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/features.9.conv: torch.nn.modules.container.Sequential/features.9.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.9.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_17: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_17 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_75, 0.0, 6.0), kwargs = {})Jä
pkg.torch.onnx.name_scopesl['', 'features', 'features.9', 'features.9.conv', 'features.9.conv.1', 'features.9.conv.1.2', 'hardtanh_17']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
œ
hardtanh_17
features.9.conv.2.weight
features.9.conv.2.weight_bias
getitem_78node_Conv_757"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†JÚ
	namespace‰: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/features.9.conv: torch.nn.modules.container.Sequential/features.9.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_26: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J“
pkg.torch.onnx.fx_node∑%_native_batch_norm_legit_no_training_26 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_26, %p_features_9_conv_3_weight, %p_features_9_conv_3_bias, %b_features_9_conv_3_running_mean, %b_features_9_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jè
pkg.torch.onnx.name_scopesq['', 'features', 'features.9', 'features.9.conv', 'features.9.conv.3', '_native_batch_norm_legit_no_training_26']Jõ
pkg.torch.onnx.stack_trace¸File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
ƒ	
add_3

getitem_78add_4
node_add_4"AddJΩ
	namespaceØ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.9: torchvision.models.mobilenetv2.InvertedResidual/add_4: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jç
pkg.torch.onnx.fx_nodes%add_4 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_3, %getitem_78), kwargs = {})JE
pkg.torch.onnx.name_scopes'['', 'features', 'features.9', 'add_4']Jº
pkg.torch.onnx.stack_traceùFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
ó
add_4
features.10.conv.0.0.weight
 features.10.conv.0.0.weight_bias
getitem_81node_Conv_759"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/features.10.conv: torch.nn.modules.container.Sequential/features.10.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.10.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_27: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_27 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_27, %p_features_10_conv_0_1_weight, %p_features_10_conv_0_1_bias, %b_features_10_conv_0_1_running_mean, %b_features_10_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.10', 'features.10.conv', 'features.10.conv.0', 'features.10.conv.0.1', '_native_batch_norm_legit_no_training_27']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
Æ

getitem_81
min_val_cast
max_val_casthardtanh_18n4_19"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/features.10.conv: torch.nn.modules.container.Sequential/features.10.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.10.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_18: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_18 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_81, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.10', 'features.10.conv', 'features.10.conv.0', 'features.10.conv.0.2', 'hardtanh_18']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
û
hardtanh_18
features.10.conv.1.0.weight
 features.10.conv.1.0.weight_bias
getitem_84node_Conv_761"Conv*
groupÄ†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/features.10.conv: torch.nn.modules.container.Sequential/features.10.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.10.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_28: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_28 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_28, %p_features_10_conv_1_1_weight, %p_features_10_conv_1_1_bias, %b_features_10_conv_1_1_running_mean, %b_features_10_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.10', 'features.10.conv', 'features.10.conv.1', 'features.10.conv.1.1', '_native_batch_norm_legit_no_training_28']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
Æ

getitem_84
min_val_cast
max_val_casthardtanh_19n4_20"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/features.10.conv: torch.nn.modules.container.Sequential/features.10.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.10.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_19: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_19 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_84, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.10', 'features.10.conv', 'features.10.conv.1', 'features.10.conv.1.2', 'hardtanh_19']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
€
hardtanh_19
features.10.conv.2.weight
features.10.conv.2.weight_bias
getitem_87node_Conv_763"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/features.10.conv: torch.nn.modules.container.Sequential/features.10.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_29: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_29 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_29, %p_features_10_conv_3_weight, %p_features_10_conv_3_bias, %b_features_10_conv_3_running_mean, %b_features_10_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.10', 'features.10.conv', 'features.10.conv.3', '_native_batch_norm_legit_no_training_29']Jõ
pkg.torch.onnx.stack_trace¸File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
∆	
add_4

getitem_87add_5
node_add_5"AddJæ
	namespace∞: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.10: torchvision.models.mobilenetv2.InvertedResidual/add_5: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jç
pkg.torch.onnx.fx_nodes%add_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_4, %getitem_87), kwargs = {})JF
pkg.torch.onnx.name_scopes(['', 'features', 'features.10', 'add_5']Jº
pkg.torch.onnx.stack_traceùFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
ì
add_5
features.11.conv.0.0.weight
 features.11.conv.0.0.weight_bias
getitem_90node_Conv_765"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.11: torchvision.models.mobilenetv2.InvertedResidual/features.11.conv: torch.nn.modules.container.Sequential/features.11.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.11.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_30: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_30 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_30, %p_features_11_conv_0_1_weight, %p_features_11_conv_0_1_bias, %b_features_11_conv_0_1_running_mean, %b_features_11_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.11', 'features.11.conv', 'features.11.conv.0', 'features.11.conv.0.1', '_native_batch_norm_legit_no_training_30']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
™

getitem_90
min_val_cast
max_val_casthardtanh_20n4_21"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.11: torchvision.models.mobilenetv2.InvertedResidual/features.11.conv: torch.nn.modules.container.Sequential/features.11.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.11.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_20: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_20 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_90, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.11', 'features.11.conv', 'features.11.conv.0', 'features.11.conv.0.2', 'hardtanh_20']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ö
hardtanh_20
features.11.conv.1.0.weight
 features.11.conv.1.0.weight_bias
getitem_93node_Conv_767"Conv*
groupÄ†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.11: torchvision.models.mobilenetv2.InvertedResidual/features.11.conv: torch.nn.modules.container.Sequential/features.11.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.11.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_31: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_31 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_31, %p_features_11_conv_1_1_weight, %p_features_11_conv_1_1_bias, %b_features_11_conv_1_1_running_mean, %b_features_11_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.11', 'features.11.conv', 'features.11.conv.1', 'features.11.conv.1.1', '_native_batch_norm_legit_no_training_31']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
™

getitem_93
min_val_cast
max_val_casthardtanh_21n4_22"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.11: torchvision.models.mobilenetv2.InvertedResidual/features.11.conv: torch.nn.modules.container.Sequential/features.11.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.11.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_21: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_21 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_93, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.11', 'features.11.conv', 'features.11.conv.1', 'features.11.conv.1.2', 'hardtanh_21']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
◊
hardtanh_21
features.11.conv.2.weight
features.11.conv.2.weight_bias
getitem_96node_Conv_769"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.11: torchvision.models.mobilenetv2.InvertedResidual/features.11.conv: torch.nn.modules.container.Sequential/features.11.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_32: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_32 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_32, %p_features_11_conv_3_weight, %p_features_11_conv_3_bias, %b_features_11_conv_3_running_mean, %b_features_11_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.11', 'features.11.conv', 'features.11.conv.3', '_native_batch_norm_legit_no_training_32']Jó
pkg.torch.onnx.stack_trace¯File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
ú

getitem_96
features.12.conv.0.0.weight
 features.12.conv.0.0.weight_bias
getitem_99node_Conv_771"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/features.12.conv: torch.nn.modules.container.Sequential/features.12.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.12.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_33: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_33 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_33, %p_features_12_conv_0_1_weight, %p_features_12_conv_0_1_bias, %b_features_12_conv_0_1_running_mean, %b_features_12_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.12', 'features.12.conv', 'features.12.conv.0', 'features.12.conv.0.1', '_native_batch_norm_legit_no_training_33']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
Æ

getitem_99
min_val_cast
max_val_casthardtanh_22n4_23"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/features.12.conv: torch.nn.modules.container.Sequential/features.12.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.12.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_22: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jú
pkg.torch.onnx.fx_nodeÅ%hardtanh_22 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_99, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.12', 'features.12.conv', 'features.12.conv.0', 'features.12.conv.0.2', 'hardtanh_22']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ü
hardtanh_22
features.12.conv.1.0.weight
 features.12.conv.1.0.weight_biasgetitem_102node_Conv_773"Conv*
group¿†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/features.12.conv: torch.nn.modules.container.Sequential/features.12.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.12.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_34: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_34 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_34, %p_features_12_conv_1_1_weight, %p_features_12_conv_1_1_bias, %b_features_12_conv_1_1_running_mean, %b_features_12_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.12', 'features.12.conv', 'features.12.conv.1', 'features.12.conv.1.1', '_native_batch_norm_legit_no_training_34']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_102
min_val_cast
max_val_casthardtanh_23n4_24"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/features.12.conv: torch.nn.modules.container.Sequential/features.12.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.12.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_23: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_23 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_102, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.12', 'features.12.conv', 'features.12.conv.1', 'features.12.conv.1.2', 'hardtanh_23']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
‹
hardtanh_23
features.12.conv.2.weight
features.12.conv.2.weight_biasgetitem_105node_Conv_775"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/features.12.conv: torch.nn.modules.container.Sequential/features.12.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_35: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_35 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_35, %p_features_12_conv_3_weight, %p_features_12_conv_3_bias, %b_features_12_conv_3_running_mean, %b_features_12_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.12', 'features.12.conv', 'features.12.conv.3', '_native_batch_norm_legit_no_training_35']Jõ
pkg.torch.onnx.stack_trace¸File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
“	

getitem_96
getitem_105add_6
node_add_6"AddJæ
	namespace∞: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.12: torchvision.models.mobilenetv2.InvertedResidual/add_6: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jì
pkg.torch.onnx.fx_nodey%add_6 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_96, %getitem_105), kwargs = {})JF
pkg.torch.onnx.name_scopes(['', 'features', 'features.12', 'add_6']Jº
pkg.torch.onnx.stack_traceùFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
ò
add_6
features.13.conv.0.0.weight
 features.13.conv.0.0.weight_biasgetitem_108node_Conv_777"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/features.13.conv: torch.nn.modules.container.Sequential/features.13.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.13.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_36: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_36 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_36, %p_features_13_conv_0_1_weight, %p_features_13_conv_0_1_bias, %b_features_13_conv_0_1_running_mean, %b_features_13_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.13', 'features.13.conv', 'features.13.conv.0', 'features.13.conv.0.1', '_native_batch_norm_legit_no_training_36']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_108
min_val_cast
max_val_casthardtanh_24n4_25"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/features.13.conv: torch.nn.modules.container.Sequential/features.13.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.13.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_24: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_24 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_108, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.13', 'features.13.conv', 'features.13.conv.0', 'features.13.conv.0.2', 'hardtanh_24']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ü
hardtanh_24
features.13.conv.1.0.weight
 features.13.conv.1.0.weight_biasgetitem_111node_Conv_779"Conv*
group¿†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/features.13.conv: torch.nn.modules.container.Sequential/features.13.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.13.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_37: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_37 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_37, %p_features_13_conv_1_1_weight, %p_features_13_conv_1_1_bias, %b_features_13_conv_1_1_running_mean, %b_features_13_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.13', 'features.13.conv', 'features.13.conv.1', 'features.13.conv.1.1', '_native_batch_norm_legit_no_training_37']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_111
min_val_cast
max_val_casthardtanh_25n4_26"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/features.13.conv: torch.nn.modules.container.Sequential/features.13.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.13.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_25: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_25 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_111, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.13', 'features.13.conv', 'features.13.conv.1', 'features.13.conv.1.2', 'hardtanh_25']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
‹
hardtanh_25
features.13.conv.2.weight
features.13.conv.2.weight_biasgetitem_114node_Conv_781"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/features.13.conv: torch.nn.modules.container.Sequential/features.13.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_38: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_38 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_38, %p_features_13_conv_3_weight, %p_features_13_conv_3_bias, %b_features_13_conv_3_running_mean, %b_features_13_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.13', 'features.13.conv', 'features.13.conv.3', '_native_batch_norm_legit_no_training_38']Jõ
pkg.torch.onnx.stack_trace¸File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
»	
add_6
getitem_114add_7
node_add_7"AddJæ
	namespace∞: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.13: torchvision.models.mobilenetv2.InvertedResidual/add_7: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_7 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_6, %getitem_114), kwargs = {})JF
pkg.torch.onnx.name_scopes(['', 'features', 'features.13', 'add_7']Jº
pkg.torch.onnx.stack_traceùFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
î
add_7
features.14.conv.0.0.weight
 features.14.conv.0.0.weight_biasgetitem_117node_Conv_783"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.14: torchvision.models.mobilenetv2.InvertedResidual/features.14.conv: torch.nn.modules.container.Sequential/features.14.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.14.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_39: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_39 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_39, %p_features_14_conv_0_1_weight, %p_features_14_conv_0_1_bias, %b_features_14_conv_0_1_running_mean, %b_features_14_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.14', 'features.14.conv', 'features.14.conv.0', 'features.14.conv.0.1', '_native_batch_norm_legit_no_training_39']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¨
getitem_117
min_val_cast
max_val_casthardtanh_26n4_27"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.14: torchvision.models.mobilenetv2.InvertedResidual/features.14.conv: torch.nn.modules.container.Sequential/features.14.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.14.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_26: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_26 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_117, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.14', 'features.14.conv', 'features.14.conv.0', 'features.14.conv.0.2', 'hardtanh_26']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
õ
hardtanh_26
features.14.conv.1.0.weight
 features.14.conv.1.0.weight_biasgetitem_120node_Conv_785"Conv*
group¿†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.14: torchvision.models.mobilenetv2.InvertedResidual/features.14.conv: torch.nn.modules.container.Sequential/features.14.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.14.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_40: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_40 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_40, %p_features_14_conv_1_1_weight, %p_features_14_conv_1_1_bias, %b_features_14_conv_1_1_running_mean, %b_features_14_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.14', 'features.14.conv', 'features.14.conv.1', 'features.14.conv.1.1', '_native_batch_norm_legit_no_training_40']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¨
getitem_120
min_val_cast
max_val_casthardtanh_27n4_28"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.14: torchvision.models.mobilenetv2.InvertedResidual/features.14.conv: torch.nn.modules.container.Sequential/features.14.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.14.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_27: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_27 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_120, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.14', 'features.14.conv', 'features.14.conv.1', 'features.14.conv.1.2', 'hardtanh_27']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ÿ
hardtanh_27
features.14.conv.2.weight
features.14.conv.2.weight_biasgetitem_123node_Conv_787"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.14: torchvision.models.mobilenetv2.InvertedResidual/features.14.conv: torch.nn.modules.container.Sequential/features.14.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_41: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_41 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_41, %p_features_14_conv_3_weight, %p_features_14_conv_3_bias, %b_features_14_conv_3_running_mean, %b_features_14_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.14', 'features.14.conv', 'features.14.conv.3', '_native_batch_norm_legit_no_training_41']Jó
pkg.torch.onnx.stack_trace¯File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
û
getitem_123
features.15.conv.0.0.weight
 features.15.conv.0.0.weight_biasgetitem_126node_Conv_789"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/features.15.conv: torch.nn.modules.container.Sequential/features.15.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.15.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_42: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_42 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_42, %p_features_15_conv_0_1_weight, %p_features_15_conv_0_1_bias, %b_features_15_conv_0_1_running_mean, %b_features_15_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.15', 'features.15.conv', 'features.15.conv.0', 'features.15.conv.0.1', '_native_batch_norm_legit_no_training_42']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_126
min_val_cast
max_val_casthardtanh_28n4_29"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/features.15.conv: torch.nn.modules.container.Sequential/features.15.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.15.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_28: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_28 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_126, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.15', 'features.15.conv', 'features.15.conv.0', 'features.15.conv.0.2', 'hardtanh_28']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ü
hardtanh_28
features.15.conv.1.0.weight
 features.15.conv.1.0.weight_biasgetitem_129node_Conv_791"Conv*
group¿†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/features.15.conv: torch.nn.modules.container.Sequential/features.15.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.15.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_43: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_43 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_43, %p_features_15_conv_1_1_weight, %p_features_15_conv_1_1_bias, %b_features_15_conv_1_1_running_mean, %b_features_15_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.15', 'features.15.conv', 'features.15.conv.1', 'features.15.conv.1.1', '_native_batch_norm_legit_no_training_43']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_129
min_val_cast
max_val_casthardtanh_29n4_30"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/features.15.conv: torch.nn.modules.container.Sequential/features.15.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.15.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_29: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_29 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_129, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.15', 'features.15.conv', 'features.15.conv.1', 'features.15.conv.1.2', 'hardtanh_29']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
‹
hardtanh_29
features.15.conv.2.weight
features.15.conv.2.weight_biasgetitem_132node_Conv_793"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/features.15.conv: torch.nn.modules.container.Sequential/features.15.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_44: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_44 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_44, %p_features_15_conv_3_weight, %p_features_15_conv_3_bias, %b_features_15_conv_3_running_mean, %b_features_15_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.15', 'features.15.conv', 'features.15.conv.3', '_native_batch_norm_legit_no_training_44']Jõ
pkg.torch.onnx.stack_trace¸File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
‘	
getitem_123
getitem_132add_8
node_add_8"AddJæ
	namespace∞: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.15: torchvision.models.mobilenetv2.InvertedResidual/add_8: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jî
pkg.torch.onnx.fx_nodez%add_8 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_123, %getitem_132), kwargs = {})JF
pkg.torch.onnx.name_scopes(['', 'features', 'features.15', 'add_8']Jº
pkg.torch.onnx.stack_traceùFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
ò
add_8
features.16.conv.0.0.weight
 features.16.conv.0.0.weight_biasgetitem_135node_Conv_795"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/features.16.conv: torch.nn.modules.container.Sequential/features.16.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.16.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_45: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_45 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_45, %p_features_16_conv_0_1_weight, %p_features_16_conv_0_1_bias, %b_features_16_conv_0_1_running_mean, %b_features_16_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.16', 'features.16.conv', 'features.16.conv.0', 'features.16.conv.0.1', '_native_batch_norm_legit_no_training_45']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_135
min_val_cast
max_val_casthardtanh_30n4_31"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/features.16.conv: torch.nn.modules.container.Sequential/features.16.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.16.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_30: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_30 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_135, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.16', 'features.16.conv', 'features.16.conv.0', 'features.16.conv.0.2', 'hardtanh_30']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ü
hardtanh_30
features.16.conv.1.0.weight
 features.16.conv.1.0.weight_biasgetitem_138node_Conv_797"Conv*
group¿†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/features.16.conv: torch.nn.modules.container.Sequential/features.16.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.16.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_46: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_46 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_46, %p_features_16_conv_1_1_weight, %p_features_16_conv_1_1_bias, %b_features_16_conv_1_1_running_mean, %b_features_16_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.16', 'features.16.conv', 'features.16.conv.1', 'features.16.conv.1.1', '_native_batch_norm_legit_no_training_46']JÀ
pkg.torch.onnx.stack_trace¨File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
∞
getitem_138
min_val_cast
max_val_casthardtanh_31n4_32"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/features.16.conv: torch.nn.modules.container.Sequential/features.16.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.16.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_31: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_31 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_138, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.16', 'features.16.conv', 'features.16.conv.1', 'features.16.conv.1.2', 'hardtanh_31']J˙
pkg.torch.onnx.stack_trace€File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
‹
hardtanh_31
features.16.conv.2.weight
features.16.conv.2.weight_biasgetitem_141node_Conv_799"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/features.16.conv: torch.nn.modules.container.Sequential/features.16.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_47: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_47 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_47, %p_features_16_conv_3_weight, %p_features_16_conv_3_bias, %b_features_16_conv_3_running_mean, %b_features_16_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.16', 'features.16.conv', 'features.16.conv.3', '_native_batch_norm_legit_no_training_47']Jõ
pkg.torch.onnx.stack_trace¸File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
»	
add_8
getitem_141add_9
node_add_9"AddJæ
	namespace∞: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.16: torchvision.models.mobilenetv2.InvertedResidual/add_9: aten.add.TensorJ¿
pkg.torch.onnx.class_hierarchyù['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_9 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_8, %getitem_141), kwargs = {})JF
pkg.torch.onnx.name_scopes(['', 'features', 'features.16', 'add_9']Jº
pkg.torch.onnx.stack_traceùFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 62, in forward
    return x + self.conv(x)
î
add_9
features.17.conv.0.0.weight
 features.17.conv.0.0.weight_biasgetitem_144node_Conv_801"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.17: torchvision.models.mobilenetv2.InvertedResidual/features.17.conv: torch.nn.modules.container.Sequential/features.17.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.17.conv.0.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_48: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_48 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_48, %p_features_17_conv_0_1_weight, %p_features_17_conv_0_1_bias, %b_features_17_conv_0_1_running_mean, %b_features_17_conv_0_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.17', 'features.17.conv', 'features.17.conv.0', 'features.17.conv.0.1', '_native_batch_norm_legit_no_training_48']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¨
getitem_144
min_val_cast
max_val_casthardtanh_32n4_33"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.17: torchvision.models.mobilenetv2.InvertedResidual/features.17.conv: torch.nn.modules.container.Sequential/features.17.conv.0: torchvision.ops.misc.Conv2dNormActivation/features.17.conv.0.2: torch.nn.modules.activation.ReLU6/hardtanh_32: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_32 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_144, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.17', 'features.17.conv', 'features.17.conv.0', 'features.17.conv.0.2', 'hardtanh_32']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
õ
hardtanh_32
features.17.conv.1.0.weight
 features.17.conv.1.0.weight_biasgetitem_147node_Conv_803"Conv*
group¿†*
pads@@@@†*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.17: torchvision.models.mobilenetv2.InvertedResidual/features.17.conv: torch.nn.modules.container.Sequential/features.17.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.17.conv.1.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_49: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJ‚
pkg.torch.onnx.class_hierarchyø['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']Jﬁ
pkg.torch.onnx.fx_node√%_native_batch_norm_legit_no_training_49 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_49, %p_features_17_conv_1_1_weight, %p_features_17_conv_1_1_bias, %b_features_17_conv_1_1_running_mean, %b_features_17_conv_1_1_running_var, 0.1, 1e-05), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'features', 'features.17', 'features.17.conv', 'features.17.conv.1', 'features.17.conv.1.1', '_native_batch_norm_legit_no_training_49']J«
pkg.torch.onnx.stack_trace®File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
¨
getitem_147
min_val_cast
max_val_casthardtanh_33n4_34"ClipJ¯
	namespaceÍ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.17: torchvision.models.mobilenetv2.InvertedResidual/features.17.conv: torch.nn.modules.container.Sequential/features.17.conv.1: torchvision.ops.misc.Conv2dNormActivation/features.17.conv.1.2: torch.nn.modules.activation.ReLU6/hardtanh_33: aten.hardtanh.defaultJ¡
pkg.torch.onnx.class_hierarchyû['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_33 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_147, 0.0, 6.0), kwargs = {})Jé
pkg.torch.onnx.name_scopesp['', 'features', 'features.17', 'features.17.conv', 'features.17.conv.1', 'features.17.conv.1.2', 'hardtanh_33']Jˆ
pkg.torch.onnx.stack_trace◊File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
ÿ
hardtanh_33
features.17.conv.2.weight
features.17.conv.2.weight_biasgetitem_150node_Conv_805"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†Jı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.17: torchvision.models.mobilenetv2.InvertedResidual/features.17.conv: torch.nn.modules.container.Sequential/features.17.conv.3: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_50: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJµ
pkg.torch.onnx.class_hierarchyí['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.models.mobilenetv2.InvertedResidual', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J÷
pkg.torch.onnx.fx_nodeª%_native_batch_norm_legit_no_training_50 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_50, %p_features_17_conv_3_weight, %p_features_17_conv_3_bias, %b_features_17_conv_3_running_mean, %b_features_17_conv_3_running_var, 0.1, 1e-05), kwargs = {})Jí
pkg.torch.onnx.name_scopest['', 'features', 'features.17', 'features.17.conv', 'features.17.conv.3', '_native_batch_norm_legit_no_training_50']Jó
pkg.torch.onnx.stack_trace¯File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 64, in forward
    return self.conv(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
˝
getitem_150
features.18.0.weight
features.18.0.weight_biasgetitem_153node_Conv_807"Conv*
group†*
pads@ @ @ @ †*
strides@@†*
auto_pad"NOTSET†*
	dilations@@†J≤
	namespace§: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.18: torchvision.ops.misc.Conv2dNormActivation/features.18.1: torch.nn.modules.batchnorm.BatchNorm2d/_native_batch_norm_legit_no_training_51: aten._native_batch_norm_legit_no_training.defaultJV
!pkg.onnxscript.rewriter.rule_name1FuseBatchNormIntoConv, RemoveOptionalBiasFromConvJÜ
pkg.torch.onnx.class_hierarchy„['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.batchnorm.BatchNorm2d', 'aten._native_batch_norm_legit_no_training.default']J¬
pkg.torch.onnx.fx_nodeß%_native_batch_norm_legit_no_training_51 : [num_users=1] = call_function[target=torch.ops.aten._native_batch_norm_legit_no_training.default](args = (%conv2d_51, %p_features_18_1_weight, %p_features_18_1_bias, %b_features_18_1_running_mean, %b_features_18_1_running_var, 0.1, 1e-05), kwargs = {})Jy
pkg.torch.onnx.name_scopes[['', 'features', 'features.18', 'features.18.1', '_native_batch_norm_legit_no_training_51']JÊ
pkg.torch.onnx.stack_trace«File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
∫
getitem_153
min_val_cast
max_val_casthardtanh_34n4_35"ClipJı
	namespaceÁ: torchvision.models.mobilenetv2.MobileNetV2/features: torch.nn.modules.container.Sequential/features.18: torchvision.ops.misc.Conv2dNormActivation/features.18.2: torch.nn.modules.activation.ReLU6/hardtanh_34: aten.hardtanh.defaultJÂ
pkg.torch.onnx.class_hierarchy¬['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torchvision.ops.misc.Conv2dNormActivation', 'torch.nn.modules.activation.ReLU6', 'aten.hardtanh.default']Jù
pkg.torch.onnx.fx_nodeÇ%hardtanh_34 : [num_users=1] = call_function[target=torch.ops.aten.hardtanh.default](args = (%getitem_153, 0.0, 6.0), kwargs = {})J]
pkg.torch.onnx.name_scopes?['', 'features', 'features.18', 'features.18.2', 'hardtanh_34']Jï
pkg.torch.onnx.stack_traceˆFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 292, in forward
    return F.hardtanh(input, self.min_val, self.max_val, self.inplace)
Ø
hardtanh_34
val_473mean	node_mean"
ReduceMean*
noop_with_empty_axes †*
keepdims†JM
	namespace@: torchvision.models.mobilenetv2.MobileNetV2/mean: aten.mean.dimJa
pkg.torch.onnx.class_hierarchy?['torchvision.models.mobilenetv2.MobileNetV2', 'aten.mean.dim']Jì
pkg.torch.onnx.fx_nodey%mean : [num_users=1] = call_function[target=torch.ops.aten.mean.dim](args = (%hardtanh_34, [-1, -2], True), kwargs = {})J*
pkg.torch.onnx.name_scopes['', 'mean']J◊
pkg.torch.onnx.stack_trace∏File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
â
mean
val_477view	node_view"Reshape*
	allowzero†JQ
	namespaceD: torchvision.models.mobilenetv2.MobileNetV2/view: aten.view.defaultJe
pkg.torch.onnx.class_hierarchyC['torchvision.models.mobilenetv2.MobileNetV2', 'aten.view.default']Jã
pkg.torch.onnx.fx_nodeq%view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%mean, [1, 1280]), kwargs = {})J*
pkg.torch.onnx.name_scopes['', 'view']J◊
pkg.torch.onnx.stack_trace∏File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
…

view
classifier.1.weight
classifier.1.biasoutputnode_linear"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †Jµ
	namespaceß: torchvision.models.mobilenetv2.MobileNetV2/classifier: torch.nn.modules.container.Sequential/classifier.1: torch.nn.modules.linear.Linear/linear: aten.linear.defaultJ≥
pkg.torch.onnx.class_hierarchyê['torchvision.models.mobilenetv2.MobileNetV2', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J¥
pkg.torch.onnx.fx_nodeô%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone, %p_classifier_1_weight, %p_classifier_1_bias), kwargs = {})JJ
pkg.torch.onnx.name_scopes,['', 'classifier', 'classifier.1', 'linear']JÕ
pkg.torch.onnx.stack_traceÆFile "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torchvision/models/mobilenetv2.py", line 174, in forward
    return self._forward_impl(x)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/Users/alexauyeung/Work/wovon/dev/image-categorization/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
main_graph*p Bfeatures.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset41344j
length3456p*w Bfeatures.1.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset10752j
length1152p*u Bfeatures.1.conv.1.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset25472j
length2048p*w`Bfeatures.2.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset44800j
length3456p*-Bclassifier.1.biasJZJ=äIΩ‘¶“Ω∂Sy=z=*w`Bfeatures.2.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset86784j
length6144p*v`Bfeatures.2.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset113664j
length9216p*zêBfeatures.3.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset122880j
length13824p*xêBfeatures.3.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset76416j
length5184p*xêBfeatures.3.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset136704j
length13824p*zêBfeatures.4.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset150528j
length13824p*xêBfeatures.4.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset81600j
length5184p*x êBfeatures.4.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset219648j
length18432p*z¿ Bfeatures.5.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset300288j
length24576p*x¿Bfeatures.5.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset92928j
length6912p*x ¿Bfeatures.5.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset324864j
length24576p*z¿ Bfeatures.6.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset349440j
length24576p*x¿Bfeatures.6.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset99840j
length6912p*x ¿Bfeatures.6.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset374016j
length24576p*z¿ Bfeatures.7.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset398592j
length24576p*y¿Bfeatures.7.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset106752j
length6912p*x@¿Bfeatures.7.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset552448j
length49152p*zÄ@Bfeatures.8.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset601600j
length98304p*zÄBfeatures.8.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset164352j
length13824p*x@ÄBfeatures.8.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset699904j
length98304p*zÄ@Bfeatures.9.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset798208j
length98304p*zÄBfeatures.9.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset178176j
length13824p*x@ÄBfeatures.9.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset896512j
length98304p*{Ä@Bfeatures.10.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset994816j
length98304p*{ÄBfeatures.10.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset192000j
length13824p*z@ÄBfeatures.10.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset1093120j
length98304p*|Ä@Bfeatures.11.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset1191424j
length98304p*{ÄBfeatures.11.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset205824j
length13824p*{`ÄBfeatures.11.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset1289728j
length147456p*}¿`Bfeatures.12.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset1437184j
length221184p*{¿Bfeatures.12.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset238080j
length20736p*{`¿Bfeatures.12.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset1658368j
length221184p*}¿`Bfeatures.13.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset1879552j
length221184p*{¿Bfeatures.13.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset258816j
length20736p*{`¿Bfeatures.13.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset2100736j
length221184p*}¿`Bfeatures.14.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset2321920j
length221184p*{¿Bfeatures.14.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset279552j
length20736p*|†¿Bfeatures.14.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset2543104j
length368640p*~¿†Bfeatures.15.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset2911744j
length614400p*{¿Bfeatures.15.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset448768j
length34560p*|†¿Bfeatures.15.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset3526144j
length614400p*~¿†Bfeatures.16.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset4140544j
length614400p*{¿Bfeatures.16.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset483328j
length34560p*|†¿Bfeatures.16.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset4754944j
length614400p*~¿†Bfeatures.17.conv.0.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset5369344j
length614400p*{¿Bfeatures.17.conv.1.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset517888j
length34560p*}¿¿Bfeatures.17.conv.2.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset6029312j
length1228800p*xÄ
¿Bfeatures.18.0.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset7274496j
length1638400p*oÄ
Bclassifier.1.weightj,
location mobilenet-fashion-5cat.onnx.dataj
offset423168j
length25600p*Bmin_val_castJ    *Bmax_val_castJ  ¿@*Bval_473Jˇˇˇˇˇˇˇˇ˛ˇˇˇˇˇˇˇ*Bval_477J              *° Bfeatures.0.0.weight_biasJÄÿ#ΩeKÓ>t∂π>ÜÖÆ>ˇ é?‹/?*˘>µ⁄ê>|>?∆_eºÚ:>pµT?¿Â>rÿ0øKVºÑÓ´ª<dΩ\ºX>à>†;Ω‘ª‡=zG#?pVgæ¸Ó°º`P=ààÔ>h2o=ÑÖY?§Pù>Fn>Î…>§◊Ì=*® Bfeatures.1.conv.0.0.weight_biasJÄ=dCº√}m?≈˛øÄøø>˘îá>ãÑS?oO?ñÄ”>Ÿ0Ñ@≠„òª˜ò§>^|¶>ﬂ≤B?zW∫æ€Íπª¥ªº|<Cˆ^<Píù<HÜD=˙yöæ¨%›?¥öT?TÄ†:…E?qh¢øo≈æy‡<?ÎL,?Ê∏ì>”rs?ÛRæ*eBfeatures.1.conv.1.weight_biasJ@r…¿®?†Û∑øD∑í?a’ΩÑ¿¸æÀ@ø¸,¿zPmøØ¯øÃ_’?~¶ø>èYø4™Üæ›π@J@*q`Bfeatures.2.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset0j
length384p*s`Bfeatures.2.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset384j
length384p*ÖBfeatures.2.conv.2.weight_biasJ`ÈöƒøÍÓ?Ω»@ø⁄/AæCı≥Ω÷4ø5RÓæ≥%õ>?øåæØqΩåîª9|ı>æ=>∆ﬁ?\◊Í?«õI?åÌ2>Áñ!æÙÏø/- >¶Gí>õÎß>œ√Y>€”>*uêBfeatures.3.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset1920j
length576p*uêBfeatures.3.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset2496j
length576p*ÖBfeatures.3.conv.2.weight_biasJ`q`îø#ÁÖøÃ1`=ç“—Ω√—†ø£∂°?¯Ê?yu´=0ÑØ=;Ó€Ω»ô¿∫·ø/.?ıúøQÛ="Ë”æQ"–>œˇæ%ö øÄ3≠>J'á?JÄm?Z¿,d®?*uêBfeatures.4.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset3072j
length576p*uêBfeatures.4.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset3648j
length576p*¶ Bfeatures.4.conv.2.weight_biasJÄ>#D?˙»?•øv??Í°3>ı®πøûŒ}æ€âæ 1‹<éßÒø>ÉE?ﬂ+‡?»B"?6Y?JI=?ì#ø©(?‹ÎòΩXh?T+©æXødæ›ó¯><¨	øŸv◊>\w`ø˛h?ñìæ⁄aäø€¶>áÑîøkDø!Næ*u¿Bfeatures.5.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset6144j
length768p*u¿Bfeatures.5.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset6912j
length768p*¶ Bfeatures.5.conv.2.weight_biasJÄã‰≤=≈¢7>bÃæ(á*?≠<æh3Ωêw>°ó¡æCø†ìû=å®=‡1.æ.^œæ_eÒæ+»<´æΩæXbñ>”æGF>6‡	?-ø ?Y€õæQi>X§≈æßœ=ùôæºK”æéo˘>ì‰>˚ä"æ◊Ò{æ˘∑>*u¿Bfeatures.6.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset7680j
length768p*u¿Bfeatures.6.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset8448j
length768p*¶ Bfeatures.6.conv.2.weight_biasJÄk’æ¿=R%Åæœ¶b>Åiå>±”å>˘“æäÀ:>`Ω4ï ?U„Ç>≥Ö>e‘?>ÅU]>ôÛÖ>≥SøÇ†,øÜ‘ÔæFçµ=ã	c?¬ñ>ìvB>yÅΩõQà>%ãΩPmøæÓ≈æ'$“>Dûèæπø‹¥ˆ>s®ßæ*u¿Bfeatures.7.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset9216j
length768p*u¿Bfeatures.7.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset9984j
length768p*¶@Bfeatures.7.conv.2.weight_biasJÄ*ÿø{ææF áø¡=éΩ9Œ	ø¢\¡Ω$ñ>£Åøû~NΩ>ÌÒ?ÛaπøºRé?ä%à=‹ø›1‰>Ú˘›=IæÎªµæ–§9æb>∂æe◊UΩ—´ø®s*?˜7‡>∑E>P	æÖàI?ı	IøõX>‰öø	†+>->MÑ?∏Á?ì&õ>Íª‘Ω9?æΩ∞Ó#ΩÄ»úø§ø1æ7∂?≠A[?kÉEøEØTø„Qø◊¸ÓªÍúG>H™æGœ8?¡{√>∫¯?∫È%> ã? ¯.?‘YÉ?P´-º7˚r?º,ëøÈPY>…d|æf;ôæ^?—Ñ?Gæ*wÄBfeatures.8.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset13184j
length1536p*wÄBfeatures.8.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset14720j
length1536p*¶@Bfeatures.8.conv.2.weight_biasJÄ–DñæÁGæ8eí=óà>Gæk“=·Ó<…ËøJ •æS^<AÉâæ⁄>ñ¨€=Wê§Ω∞»?=§Íé>çé>Ûµ=¡À•>kïêæyaAæ†◊‰æ„<gº¸>;≈>LjræcóÄ?ƒj0æœ≠Q>Zﬁ—=}®>ËA-?µ√Ω¸e>è^Öø’+«æŸU(?±‰)Ωf>®—jæ∂ìÈΩh\©æn¨†>:O©Ω'ø?Ÿ€
>»ÊNºó72?”1.>ã«2>âc≤Ω]|£>¡>Unjæóæûë¸>™¢¢>‰ÓSΩië>ªOO?W?Z?){ ø<Xππ«RÅæ*wÄBfeatures.9.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset16256j
length1536p*wÄBfeatures.9.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset17792j
length1536p*¶@Bfeatures.9.conv.2.weight_biasJÄóæú>ı >â√®æ‡FæEàæ}êæ¬Ñ>›Ö≤=Ä¸æ∫®=‰,>õ…âæXb∞Ωˆiôæ ìû=Ù™ñ>x !>Ê€ñæ/îÈ>‚|e=S˛ö>+Ò>∑ﬂè>ÌTuΩÚ›|>ú;¡Ω=ª->”–˘ºÖ9=Q£N;?ìë:∞¶ñΩπŒ€ºuÍÿæ≠‹C>iWæ≈2MælP›æ…>¸º>k‹$æjEkæ˚{B>02>ÅÇ*>Â}R>û4ØΩ/ù>".Ræ'˙:<ï+w=≈i‡æ0Húæ01æΩ%°>ΩXûæ<õÑ;ïÉ.=∏–DæÌfæ‰kv?”u€>≤£[>i∞4æ*xÄB features.10.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset19328j
length1536p*xÄB features.10.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset20864j
length1536p*ß@Bfeatures.10.conv.2.weight_biasJÄ\Fåæ≈æœ=>}Søô8?Å‰Ω6À$æ¶>}>H‚'ødﬁ?%âΩdk˛æ)ˆæŸ0˛æÃÇƒæ◊ÃÉæ¡®f>;¢&æ·÷~>v⁄>X°±=F<>Åøç*%>Æ„ÈºÂ>ØΩ<ƒì>îÄÌ=nC*æ«Õiæs?Ω9^ø[ó>ø0sh>Äqó>tÉ=jûwΩˇ*æ‡æ∫G>Q˘>á46<ã+∂ªèSª=CÅ±ΩH9æ$÷[æ3ë∑æe¨\>{Æßæj\!ΩÜ+Ω∑›∞æó|à>ì»%=õ.èΩ"\>ŸÎöæº¶Kø>¢æÕe=o˛∂æF"NΩÛ¨F>*xÄB features.11.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset22400j
length1536p*xÄB features.11.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset23936j
length1536p*r`Bfeatures.11.conv.2.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset768j
length384p*x¿B features.12.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset27520j
length2304p*x¿B features.12.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset29824j
length2304p*s`Bfeatures.12.conv.2.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset1152j
length384p*x¿B features.13.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset32128j
length2304p*x¿B features.13.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset34432j
length2304p*s`Bfeatures.13.conv.2.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset1536j
length384p*x¿B features.14.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset36736j
length2304p*x¿B features.14.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset39040j
length2304p*t†Bfeatures.14.conv.2.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset4224j
length640p*x¿B features.15.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset48256j
length3840p*x¿B features.15.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset52096j
length3840p*t†Bfeatures.15.conv.2.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset4864j
length640p*x¿B features.16.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset55936j
length3840p*x¿B features.16.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset59776j
length3840p*t†Bfeatures.16.conv.2.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset5504j
length640p*x¿B features.17.conv.0.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset63616j
length3840p*x¿B features.17.conv.1.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset67456j
length3840p*v¿Bfeatures.17.conv.2.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset11904j
length1280p*qÄ
Bfeatures.18.0.weight_biasj,
location mobilenet-fashion-5cat.onnx.dataj
offset71296j
length5120pZ«
input



‡
‡"=
/pkg.torch.export.graph_signature.InputSpec.kind
USER_INPUT"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"&
!pkg.torch.onnx.original_node_namexbÜ
output


"?
0pkg.torch.export.graph_signature.OutputSpec.kindUSER_OUTPUT"+
!pkg.torch.onnx.original_node_namelinearj-
features.0.0.weight

 


j4
features.1.conv.0.0.weight

 


j2
features.1.conv.1.weight


 

j4
features.2.conv.1.0.weight

`


j÷
classifier.1.bias


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"8
!pkg.torch.onnx.original_node_namep_classifier_1_biasj4
features.2.conv.0.0.weight

`


j2
features.2.conv.2.weight


`

j5
features.3.conv.0.0.weight

ê


j5
features.3.conv.1.0.weight

ê


j3
features.3.conv.2.weight


ê

j5
features.4.conv.0.0.weight

ê


j5
features.4.conv.1.0.weight

ê


j3
features.4.conv.2.weight

 
ê

j5
features.5.conv.0.0.weight

¿
 

j5
features.5.conv.1.0.weight

¿


j3
features.5.conv.2.weight

 
¿

j5
features.6.conv.0.0.weight

¿
 

j5
features.6.conv.1.0.weight

¿


j3
features.6.conv.2.weight

 
¿

j5
features.7.conv.0.0.weight

¿
 

j5
features.7.conv.1.0.weight

¿


j3
features.7.conv.2.weight

@
¿

j5
features.8.conv.0.0.weight

Ä
@

j5
features.8.conv.1.0.weight

Ä


j3
features.8.conv.2.weight

@
Ä

j5
features.9.conv.0.0.weight

Ä
@

j5
features.9.conv.1.0.weight

Ä


j3
features.9.conv.2.weight

@
Ä

j6
features.10.conv.0.0.weight

Ä
@

j6
features.10.conv.1.0.weight

Ä


j4
features.10.conv.2.weight

@
Ä

j6
features.11.conv.0.0.weight

Ä
@

j6
features.11.conv.1.0.weight

Ä


j4
features.11.conv.2.weight

`
Ä

j6
features.12.conv.0.0.weight

¿
`

j6
features.12.conv.1.0.weight

¿


j4
features.12.conv.2.weight

`
¿

j6
features.13.conv.0.0.weight

¿
`

j6
features.13.conv.1.0.weight

¿


j4
features.13.conv.2.weight

`
¿

j6
features.14.conv.0.0.weight

¿
`

j6
features.14.conv.1.0.weight

¿


j5
features.14.conv.2.weight

†
¿

j7
features.15.conv.0.0.weight

¿
†

j6
features.15.conv.1.0.weight

¿


j5
features.15.conv.2.weight

†
¿

j7
features.16.conv.0.0.weight

¿
†

j6
features.16.conv.1.0.weight

¿


j5
features.16.conv.2.weight

†
¿

j7
features.17.conv.0.0.weight

¿
†

j6
features.17.conv.1.0.weight

¿


j5
features.17.conv.2.weight

¿
¿

j0
features.18.0.weight

Ä

¿

jﬂ
classifier.1.weight
	

Ä
"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone":
!pkg.torch.onnx.original_node_namep_classifier_1_weightjK
min_val_cast
 "3
$pkg.onnxscript.optimizer.folded_from['min_val']jK
max_val_cast
 "3
$pkg.onnxscript.optimizer.folded_from['max_val']jU
val_473


">
$pkg.onnxscript.optimizer.folded_from['val_471', 'val_472']jU
val_477


">
$pkg.onnxscript.optimizer.folded_from['val_475', 'val_476']j&
features.0.0.weight_bias


 j-
features.1.conv.0.0.weight_bias


 j+
features.1.conv.1.weight_bias


j-
features.2.conv.0.0.weight_bias


`j-
features.2.conv.1.0.weight_bias


`j+
features.2.conv.2.weight_bias


j.
features.3.conv.0.0.weight_bias
	
êj.
features.3.conv.1.0.weight_bias
	
êj+
features.3.conv.2.weight_bias


j.
features.4.conv.0.0.weight_bias
	
êj.
features.4.conv.1.0.weight_bias
	
êj+
features.4.conv.2.weight_bias


 j.
features.5.conv.0.0.weight_bias
	
¿j.
features.5.conv.1.0.weight_bias
	
¿j+
features.5.conv.2.weight_bias


 j.
features.6.conv.0.0.weight_bias
	
¿j.
features.6.conv.1.0.weight_bias
	
¿j+
features.6.conv.2.weight_bias


 j.
features.7.conv.0.0.weight_bias
	
¿j.
features.7.conv.1.0.weight_bias
	
¿j+
features.7.conv.2.weight_bias


@j.
features.8.conv.0.0.weight_bias
	
Äj.
features.8.conv.1.0.weight_bias
	
Äj+
features.8.conv.2.weight_bias


@j.
features.9.conv.0.0.weight_bias
	
Äj.
features.9.conv.1.0.weight_bias
	
Äj+
features.9.conv.2.weight_bias


@j/
 features.10.conv.0.0.weight_bias
	
Äj/
 features.10.conv.1.0.weight_bias
	
Äj,
features.10.conv.2.weight_bias


@j/
 features.11.conv.0.0.weight_bias
	
Äj/
 features.11.conv.1.0.weight_bias
	
Äj,
features.11.conv.2.weight_bias


`j/
 features.12.conv.0.0.weight_bias
	
¿j/
 features.12.conv.1.0.weight_bias
	
¿j,
features.12.conv.2.weight_bias


`j/
 features.13.conv.0.0.weight_bias
	
¿j/
 features.13.conv.1.0.weight_bias
	
¿j,
features.13.conv.2.weight_bias


`j/
 features.14.conv.0.0.weight_bias
	
¿j/
 features.14.conv.1.0.weight_bias
	
¿j-
features.14.conv.2.weight_bias
	
†j/
 features.15.conv.0.0.weight_bias
	
¿j/
 features.15.conv.1.0.weight_bias
	
¿j-
features.15.conv.2.weight_bias
	
†j/
 features.16.conv.0.0.weight_bias
	
¿j/
 features.16.conv.1.0.weight_bias
	
¿j-
features.16.conv.2.weight_bias
	
†j/
 features.17.conv.0.0.weight_bias
	
¿j/
 features.17.conv.1.0.weight_bias
	
¿j-
features.17.conv.2.weight_bias
	
¿j(
features.18.0.weight_bias
	
Ä
j!
getitem


 
p
pj"
hardtanh


 
p
pj#
	getitem_3


 
p
pj$

hardtanh_1


 
p
pj#
	getitem_6



p
pj#
	getitem_9


`
p
pj$

hardtanh_2


`
p
pj$

getitem_12


`
8
8j$

hardtanh_3


`
8
8j$

getitem_15



8
8j%

getitem_18


ê
8
8j%

hardtanh_4


ê
8
8j%

getitem_21


ê
8
8j%

hardtanh_5


ê
8
8j$

getitem_24



8
8j
add



8
8j%

getitem_27


ê
8
8j%

hardtanh_6


ê
8
8j%

getitem_30


ê

j%

hardtanh_7


ê

j$

getitem_33


 

j%

getitem_36


¿

j%

hardtanh_8


¿

j%

getitem_39


¿

j%

hardtanh_9


¿

j$

getitem_42


 

j
add_1


 

j%

getitem_45


¿

j&
hardtanh_10


¿

j%

getitem_48


¿

j&
hardtanh_11


¿

j$

getitem_51


 

j
add_2


 

j%

getitem_54


¿

j&
hardtanh_12


¿

j%

getitem_57


¿

j&
hardtanh_13


¿

j$

getitem_60


@

j%

getitem_63


Ä

j&
hardtanh_14


Ä

j%

getitem_66


Ä

j&
hardtanh_15


Ä

j$

getitem_69


@

j
add_3


@

j%

getitem_72


Ä

j&
hardtanh_16


Ä

j%

getitem_75


Ä

j&
hardtanh_17


Ä

j$

getitem_78


@

j
add_4


@

j%

getitem_81


Ä

j&
hardtanh_18


Ä

j%

getitem_84


Ä

j&
hardtanh_19


Ä

j$

getitem_87


@

j
add_5


@

j%

getitem_90


Ä

j&
hardtanh_20


Ä

j%

getitem_93


Ä

j&
hardtanh_21


Ä

j$

getitem_96


`

j%

getitem_99


¿

j&
hardtanh_22


¿

j&
getitem_102


¿

j&
hardtanh_23


¿

j%
getitem_105


`

j
add_6


`

j&
getitem_108


¿

j&
hardtanh_24


¿

j&
getitem_111


¿

j&
hardtanh_25


¿

j%
getitem_114


`

j
add_7


`

j&
getitem_117


¿

j&
hardtanh_26


¿

j&
getitem_120


¿

j&
hardtanh_27


¿

j&
getitem_123


†

j&
getitem_126


¿

j&
hardtanh_28


¿

j&
getitem_129


¿

j&
hardtanh_29


¿

j&
getitem_132


†

j 
add_8


†

j&
getitem_135


¿

j&
hardtanh_30


¿

j&
getitem_138


¿

j&
hardtanh_31


¿

j&
getitem_141


†

j 
add_9


†

j&
getitem_144


¿

j&
hardtanh_32


¿

j&
getitem_147


¿

j&
hardtanh_33


¿

j&
getitem_150


¿

j&
getitem_153


Ä


j&
hardtanh_34


Ä


j
mean


Ä


j
view
	

Ä
ÇÕ⁄
0pkg.torch.export.ExportedProgram.graph_signatureó⁄
# inputs
p_features_0_0_weight: PARAMETER target='features.0.0.weight'
p_features_0_1_weight: PARAMETER target='features.0.1.weight'
p_features_0_1_bias: PARAMETER target='features.0.1.bias'
p_features_1_conv_0_0_weight: PARAMETER target='features.1.conv.0.0.weight'
p_features_1_conv_0_1_weight: PARAMETER target='features.1.conv.0.1.weight'
p_features_1_conv_0_1_bias: PARAMETER target='features.1.conv.0.1.bias'
p_features_1_conv_1_weight: PARAMETER target='features.1.conv.1.weight'
p_features_1_conv_2_weight: PARAMETER target='features.1.conv.2.weight'
p_features_1_conv_2_bias: PARAMETER target='features.1.conv.2.bias'
p_features_2_conv_0_0_weight: PARAMETER target='features.2.conv.0.0.weight'
p_features_2_conv_0_1_weight: PARAMETER target='features.2.conv.0.1.weight'
p_features_2_conv_0_1_bias: PARAMETER target='features.2.conv.0.1.bias'
p_features_2_conv_1_0_weight: PARAMETER target='features.2.conv.1.0.weight'
p_features_2_conv_1_1_weight: PARAMETER target='features.2.conv.1.1.weight'
p_features_2_conv_1_1_bias: PARAMETER target='features.2.conv.1.1.bias'
p_features_2_conv_2_weight: PARAMETER target='features.2.conv.2.weight'
p_features_2_conv_3_weight: PARAMETER target='features.2.conv.3.weight'
p_features_2_conv_3_bias: PARAMETER target='features.2.conv.3.bias'
p_features_3_conv_0_0_weight: PARAMETER target='features.3.conv.0.0.weight'
p_features_3_conv_0_1_weight: PARAMETER target='features.3.conv.0.1.weight'
p_features_3_conv_0_1_bias: PARAMETER target='features.3.conv.0.1.bias'
p_features_3_conv_1_0_weight: PARAMETER target='features.3.conv.1.0.weight'
p_features_3_conv_1_1_weight: PARAMETER target='features.3.conv.1.1.weight'
p_features_3_conv_1_1_bias: PARAMETER target='features.3.conv.1.1.bias'
p_features_3_conv_2_weight: PARAMETER target='features.3.conv.2.weight'
p_features_3_conv_3_weight: PARAMETER target='features.3.conv.3.weight'
p_features_3_conv_3_bias: PARAMETER target='features.3.conv.3.bias'
p_features_4_conv_0_0_weight: PARAMETER target='features.4.conv.0.0.weight'
p_features_4_conv_0_1_weight: PARAMETER target='features.4.conv.0.1.weight'
p_features_4_conv_0_1_bias: PARAMETER target='features.4.conv.0.1.bias'
p_features_4_conv_1_0_weight: PARAMETER target='features.4.conv.1.0.weight'
p_features_4_conv_1_1_weight: PARAMETER target='features.4.conv.1.1.weight'
p_features_4_conv_1_1_bias: PARAMETER target='features.4.conv.1.1.bias'
p_features_4_conv_2_weight: PARAMETER target='features.4.conv.2.weight'
p_features_4_conv_3_weight: PARAMETER target='features.4.conv.3.weight'
p_features_4_conv_3_bias: PARAMETER target='features.4.conv.3.bias'
p_features_5_conv_0_0_weight: PARAMETER target='features.5.conv.0.0.weight'
p_features_5_conv_0_1_weight: PARAMETER target='features.5.conv.0.1.weight'
p_features_5_conv_0_1_bias: PARAMETER target='features.5.conv.0.1.bias'
p_features_5_conv_1_0_weight: PARAMETER target='features.5.conv.1.0.weight'
p_features_5_conv_1_1_weight: PARAMETER target='features.5.conv.1.1.weight'
p_features_5_conv_1_1_bias: PARAMETER target='features.5.conv.1.1.bias'
p_features_5_conv_2_weight: PARAMETER target='features.5.conv.2.weight'
p_features_5_conv_3_weight: PARAMETER target='features.5.conv.3.weight'
p_features_5_conv_3_bias: PARAMETER target='features.5.conv.3.bias'
p_features_6_conv_0_0_weight: PARAMETER target='features.6.conv.0.0.weight'
p_features_6_conv_0_1_weight: PARAMETER target='features.6.conv.0.1.weight'
p_features_6_conv_0_1_bias: PARAMETER target='features.6.conv.0.1.bias'
p_features_6_conv_1_0_weight: PARAMETER target='features.6.conv.1.0.weight'
p_features_6_conv_1_1_weight: PARAMETER target='features.6.conv.1.1.weight'
p_features_6_conv_1_1_bias: PARAMETER target='features.6.conv.1.1.bias'
p_features_6_conv_2_weight: PARAMETER target='features.6.conv.2.weight'
p_features_6_conv_3_weight: PARAMETER target='features.6.conv.3.weight'
p_features_6_conv_3_bias: PARAMETER target='features.6.conv.3.bias'
p_features_7_conv_0_0_weight: PARAMETER target='features.7.conv.0.0.weight'
p_features_7_conv_0_1_weight: PARAMETER target='features.7.conv.0.1.weight'
p_features_7_conv_0_1_bias: PARAMETER target='features.7.conv.0.1.bias'
p_features_7_conv_1_0_weight: PARAMETER target='features.7.conv.1.0.weight'
p_features_7_conv_1_1_weight: PARAMETER target='features.7.conv.1.1.weight'
p_features_7_conv_1_1_bias: PARAMETER target='features.7.conv.1.1.bias'
p_features_7_conv_2_weight: PARAMETER target='features.7.conv.2.weight'
p_features_7_conv_3_weight: PARAMETER target='features.7.conv.3.weight'
p_features_7_conv_3_bias: PARAMETER target='features.7.conv.3.bias'
p_features_8_conv_0_0_weight: PARAMETER target='features.8.conv.0.0.weight'
p_features_8_conv_0_1_weight: PARAMETER target='features.8.conv.0.1.weight'
p_features_8_conv_0_1_bias: PARAMETER target='features.8.conv.0.1.bias'
p_features_8_conv_1_0_weight: PARAMETER target='features.8.conv.1.0.weight'
p_features_8_conv_1_1_weight: PARAMETER target='features.8.conv.1.1.weight'
p_features_8_conv_1_1_bias: PARAMETER target='features.8.conv.1.1.bias'
p_features_8_conv_2_weight: PARAMETER target='features.8.conv.2.weight'
p_features_8_conv_3_weight: PARAMETER target='features.8.conv.3.weight'
p_features_8_conv_3_bias: PARAMETER target='features.8.conv.3.bias'
p_features_9_conv_0_0_weight: PARAMETER target='features.9.conv.0.0.weight'
p_features_9_conv_0_1_weight: PARAMETER target='features.9.conv.0.1.weight'
p_features_9_conv_0_1_bias: PARAMETER target='features.9.conv.0.1.bias'
p_features_9_conv_1_0_weight: PARAMETER target='features.9.conv.1.0.weight'
p_features_9_conv_1_1_weight: PARAMETER target='features.9.conv.1.1.weight'
p_features_9_conv_1_1_bias: PARAMETER target='features.9.conv.1.1.bias'
p_features_9_conv_2_weight: PARAMETER target='features.9.conv.2.weight'
p_features_9_conv_3_weight: PARAMETER target='features.9.conv.3.weight'
p_features_9_conv_3_bias: PARAMETER target='features.9.conv.3.bias'
p_features_10_conv_0_0_weight: PARAMETER target='features.10.conv.0.0.weight'
p_features_10_conv_0_1_weight: PARAMETER target='features.10.conv.0.1.weight'
p_features_10_conv_0_1_bias: PARAMETER target='features.10.conv.0.1.bias'
p_features_10_conv_1_0_weight: PARAMETER target='features.10.conv.1.0.weight'
p_features_10_conv_1_1_weight: PARAMETER target='features.10.conv.1.1.weight'
p_features_10_conv_1_1_bias: PARAMETER target='features.10.conv.1.1.bias'
p_features_10_conv_2_weight: PARAMETER target='features.10.conv.2.weight'
p_features_10_conv_3_weight: PARAMETER target='features.10.conv.3.weight'
p_features_10_conv_3_bias: PARAMETER target='features.10.conv.3.bias'
p_features_11_conv_0_0_weight: PARAMETER target='features.11.conv.0.0.weight'
p_features_11_conv_0_1_weight: PARAMETER target='features.11.conv.0.1.weight'
p_features_11_conv_0_1_bias: PARAMETER target='features.11.conv.0.1.bias'
p_features_11_conv_1_0_weight: PARAMETER target='features.11.conv.1.0.weight'
p_features_11_conv_1_1_weight: PARAMETER target='features.11.conv.1.1.weight'
p_features_11_conv_1_1_bias: PARAMETER target='features.11.conv.1.1.bias'
p_features_11_conv_2_weight: PARAMETER target='features.11.conv.2.weight'
p_features_11_conv_3_weight: PARAMETER target='features.11.conv.3.weight'
p_features_11_conv_3_bias: PARAMETER target='features.11.conv.3.bias'
p_features_12_conv_0_0_weight: PARAMETER target='features.12.conv.0.0.weight'
p_features_12_conv_0_1_weight: PARAMETER target='features.12.conv.0.1.weight'
p_features_12_conv_0_1_bias: PARAMETER target='features.12.conv.0.1.bias'
p_features_12_conv_1_0_weight: PARAMETER target='features.12.conv.1.0.weight'
p_features_12_conv_1_1_weight: PARAMETER target='features.12.conv.1.1.weight'
p_features_12_conv_1_1_bias: PARAMETER target='features.12.conv.1.1.bias'
p_features_12_conv_2_weight: PARAMETER target='features.12.conv.2.weight'
p_features_12_conv_3_weight: PARAMETER target='features.12.conv.3.weight'
p_features_12_conv_3_bias: PARAMETER target='features.12.conv.3.bias'
p_features_13_conv_0_0_weight: PARAMETER target='features.13.conv.0.0.weight'
p_features_13_conv_0_1_weight: PARAMETER target='features.13.conv.0.1.weight'
p_features_13_conv_0_1_bias: PARAMETER target='features.13.conv.0.1.bias'
p_features_13_conv_1_0_weight: PARAMETER target='features.13.conv.1.0.weight'
p_features_13_conv_1_1_weight: PARAMETER target='features.13.conv.1.1.weight'
p_features_13_conv_1_1_bias: PARAMETER target='features.13.conv.1.1.bias'
p_features_13_conv_2_weight: PARAMETER target='features.13.conv.2.weight'
p_features_13_conv_3_weight: PARAMETER target='features.13.conv.3.weight'
p_features_13_conv_3_bias: PARAMETER target='features.13.conv.3.bias'
p_features_14_conv_0_0_weight: PARAMETER target='features.14.conv.0.0.weight'
p_features_14_conv_0_1_weight: PARAMETER target='features.14.conv.0.1.weight'
p_features_14_conv_0_1_bias: PARAMETER target='features.14.conv.0.1.bias'
p_features_14_conv_1_0_weight: PARAMETER target='features.14.conv.1.0.weight'
p_features_14_conv_1_1_weight: PARAMETER target='features.14.conv.1.1.weight'
p_features_14_conv_1_1_bias: PARAMETER target='features.14.conv.1.1.bias'
p_features_14_conv_2_weight: PARAMETER target='features.14.conv.2.weight'
p_features_14_conv_3_weight: PARAMETER target='features.14.conv.3.weight'
p_features_14_conv_3_bias: PARAMETER target='features.14.conv.3.bias'
p_features_15_conv_0_0_weight: PARAMETER target='features.15.conv.0.0.weight'
p_features_15_conv_0_1_weight: PARAMETER target='features.15.conv.0.1.weight'
p_features_15_conv_0_1_bias: PARAMETER target='features.15.conv.0.1.bias'
p_features_15_conv_1_0_weight: PARAMETER target='features.15.conv.1.0.weight'
p_features_15_conv_1_1_weight: PARAMETER target='features.15.conv.1.1.weight'
p_features_15_conv_1_1_bias: PARAMETER target='features.15.conv.1.1.bias'
p_features_15_conv_2_weight: PARAMETER target='features.15.conv.2.weight'
p_features_15_conv_3_weight: PARAMETER target='features.15.conv.3.weight'
p_features_15_conv_3_bias: PARAMETER target='features.15.conv.3.bias'
p_features_16_conv_0_0_weight: PARAMETER target='features.16.conv.0.0.weight'
p_features_16_conv_0_1_weight: PARAMETER target='features.16.conv.0.1.weight'
p_features_16_conv_0_1_bias: PARAMETER target='features.16.conv.0.1.bias'
p_features_16_conv_1_0_weight: PARAMETER target='features.16.conv.1.0.weight'
p_features_16_conv_1_1_weight: PARAMETER target='features.16.conv.1.1.weight'
p_features_16_conv_1_1_bias: PARAMETER target='features.16.conv.1.1.bias'
p_features_16_conv_2_weight: PARAMETER target='features.16.conv.2.weight'
p_features_16_conv_3_weight: PARAMETER target='features.16.conv.3.weight'
p_features_16_conv_3_bias: PARAMETER target='features.16.conv.3.bias'
p_features_17_conv_0_0_weight: PARAMETER target='features.17.conv.0.0.weight'
p_features_17_conv_0_1_weight: PARAMETER target='features.17.conv.0.1.weight'
p_features_17_conv_0_1_bias: PARAMETER target='features.17.conv.0.1.bias'
p_features_17_conv_1_0_weight: PARAMETER target='features.17.conv.1.0.weight'
p_features_17_conv_1_1_weight: PARAMETER target='features.17.conv.1.1.weight'
p_features_17_conv_1_1_bias: PARAMETER target='features.17.conv.1.1.bias'
p_features_17_conv_2_weight: PARAMETER target='features.17.conv.2.weight'
p_features_17_conv_3_weight: PARAMETER target='features.17.conv.3.weight'
p_features_17_conv_3_bias: PARAMETER target='features.17.conv.3.bias'
p_features_18_0_weight: PARAMETER target='features.18.0.weight'
p_features_18_1_weight: PARAMETER target='features.18.1.weight'
p_features_18_1_bias: PARAMETER target='features.18.1.bias'
p_classifier_1_weight: PARAMETER target='classifier.1.weight'
p_classifier_1_bias: PARAMETER target='classifier.1.bias'
b_features_0_1_running_mean: BUFFER target='features.0.1.running_mean' persistent=True
b_features_0_1_running_var: BUFFER target='features.0.1.running_var' persistent=True
b_features_0_1_num_batches_tracked: BUFFER target='features.0.1.num_batches_tracked' persistent=True
b_features_1_conv_0_1_running_mean: BUFFER target='features.1.conv.0.1.running_mean' persistent=True
b_features_1_conv_0_1_running_var: BUFFER target='features.1.conv.0.1.running_var' persistent=True
b_features_1_conv_0_1_num_batches_tracked: BUFFER target='features.1.conv.0.1.num_batches_tracked' persistent=True
b_features_1_conv_2_running_mean: BUFFER target='features.1.conv.2.running_mean' persistent=True
b_features_1_conv_2_running_var: BUFFER target='features.1.conv.2.running_var' persistent=True
b_features_1_conv_2_num_batches_tracked: BUFFER target='features.1.conv.2.num_batches_tracked' persistent=True
b_features_2_conv_0_1_running_mean: BUFFER target='features.2.conv.0.1.running_mean' persistent=True
b_features_2_conv_0_1_running_var: BUFFER target='features.2.conv.0.1.running_var' persistent=True
b_features_2_conv_0_1_num_batches_tracked: BUFFER target='features.2.conv.0.1.num_batches_tracked' persistent=True
b_features_2_conv_1_1_running_mean: BUFFER target='features.2.conv.1.1.running_mean' persistent=True
b_features_2_conv_1_1_running_var: BUFFER target='features.2.conv.1.1.running_var' persistent=True
b_features_2_conv_1_1_num_batches_tracked: BUFFER target='features.2.conv.1.1.num_batches_tracked' persistent=True
b_features_2_conv_3_running_mean: BUFFER target='features.2.conv.3.running_mean' persistent=True
b_features_2_conv_3_running_var: BUFFER target='features.2.conv.3.running_var' persistent=True
b_features_2_conv_3_num_batches_tracked: BUFFER target='features.2.conv.3.num_batches_tracked' persistent=True
b_features_3_conv_0_1_running_mean: BUFFER target='features.3.conv.0.1.running_mean' persistent=True
b_features_3_conv_0_1_running_var: BUFFER target='features.3.conv.0.1.running_var' persistent=True
b_features_3_conv_0_1_num_batches_tracked: BUFFER target='features.3.conv.0.1.num_batches_tracked' persistent=True
b_features_3_conv_1_1_running_mean: BUFFER target='features.3.conv.1.1.running_mean' persistent=True
b_features_3_conv_1_1_running_var: BUFFER target='features.3.conv.1.1.running_var' persistent=True
b_features_3_conv_1_1_num_batches_tracked: BUFFER target='features.3.conv.1.1.num_batches_tracked' persistent=True
b_features_3_conv_3_running_mean: BUFFER target='features.3.conv.3.running_mean' persistent=True
b_features_3_conv_3_running_var: BUFFER target='features.3.conv.3.running_var' persistent=True
b_features_3_conv_3_num_batches_tracked: BUFFER target='features.3.conv.3.num_batches_tracked' persistent=True
b_features_4_conv_0_1_running_mean: BUFFER target='features.4.conv.0.1.running_mean' persistent=True
b_features_4_conv_0_1_running_var: BUFFER target='features.4.conv.0.1.running_var' persistent=True
b_features_4_conv_0_1_num_batches_tracked: BUFFER target='features.4.conv.0.1.num_batches_tracked' persistent=True
b_features_4_conv_1_1_running_mean: BUFFER target='features.4.conv.1.1.running_mean' persistent=True
b_features_4_conv_1_1_running_var: BUFFER target='features.4.conv.1.1.running_var' persistent=True
b_features_4_conv_1_1_num_batches_tracked: BUFFER target='features.4.conv.1.1.num_batches_tracked' persistent=True
b_features_4_conv_3_running_mean: BUFFER target='features.4.conv.3.running_mean' persistent=True
b_features_4_conv_3_running_var: BUFFER target='features.4.conv.3.running_var' persistent=True
b_features_4_conv_3_num_batches_tracked: BUFFER target='features.4.conv.3.num_batches_tracked' persistent=True
b_features_5_conv_0_1_running_mean: BUFFER target='features.5.conv.0.1.running_mean' persistent=True
b_features_5_conv_0_1_running_var: BUFFER target='features.5.conv.0.1.running_var' persistent=True
b_features_5_conv_0_1_num_batches_tracked: BUFFER target='features.5.conv.0.1.num_batches_tracked' persistent=True
b_features_5_conv_1_1_running_mean: BUFFER target='features.5.conv.1.1.running_mean' persistent=True
b_features_5_conv_1_1_running_var: BUFFER target='features.5.conv.1.1.running_var' persistent=True
b_features_5_conv_1_1_num_batches_tracked: BUFFER target='features.5.conv.1.1.num_batches_tracked' persistent=True
b_features_5_conv_3_running_mean: BUFFER target='features.5.conv.3.running_mean' persistent=True
b_features_5_conv_3_running_var: BUFFER target='features.5.conv.3.running_var' persistent=True
b_features_5_conv_3_num_batches_tracked: BUFFER target='features.5.conv.3.num_batches_tracked' persistent=True
b_features_6_conv_0_1_running_mean: BUFFER target='features.6.conv.0.1.running_mean' persistent=True
b_features_6_conv_0_1_running_var: BUFFER target='features.6.conv.0.1.running_var' persistent=True
b_features_6_conv_0_1_num_batches_tracked: BUFFER target='features.6.conv.0.1.num_batches_tracked' persistent=True
b_features_6_conv_1_1_running_mean: BUFFER target='features.6.conv.1.1.running_mean' persistent=True
b_features_6_conv_1_1_running_var: BUFFER target='features.6.conv.1.1.running_var' persistent=True
b_features_6_conv_1_1_num_batches_tracked: BUFFER target='features.6.conv.1.1.num_batches_tracked' persistent=True
b_features_6_conv_3_running_mean: BUFFER target='features.6.conv.3.running_mean' persistent=True
b_features_6_conv_3_running_var: BUFFER target='features.6.conv.3.running_var' persistent=True
b_features_6_conv_3_num_batches_tracked: BUFFER target='features.6.conv.3.num_batches_tracked' persistent=True
b_features_7_conv_0_1_running_mean: BUFFER target='features.7.conv.0.1.running_mean' persistent=True
b_features_7_conv_0_1_running_var: BUFFER target='features.7.conv.0.1.running_var' persistent=True
b_features_7_conv_0_1_num_batches_tracked: BUFFER target='features.7.conv.0.1.num_batches_tracked' persistent=True
b_features_7_conv_1_1_running_mean: BUFFER target='features.7.conv.1.1.running_mean' persistent=True
b_features_7_conv_1_1_running_var: BUFFER target='features.7.conv.1.1.running_var' persistent=True
b_features_7_conv_1_1_num_batches_tracked: BUFFER target='features.7.conv.1.1.num_batches_tracked' persistent=True
b_features_7_conv_3_running_mean: BUFFER target='features.7.conv.3.running_mean' persistent=True
b_features_7_conv_3_running_var: BUFFER target='features.7.conv.3.running_var' persistent=True
b_features_7_conv_3_num_batches_tracked: BUFFER target='features.7.conv.3.num_batches_tracked' persistent=True
b_features_8_conv_0_1_running_mean: BUFFER target='features.8.conv.0.1.running_mean' persistent=True
b_features_8_conv_0_1_running_var: BUFFER target='features.8.conv.0.1.running_var' persistent=True
b_features_8_conv_0_1_num_batches_tracked: BUFFER target='features.8.conv.0.1.num_batches_tracked' persistent=True
b_features_8_conv_1_1_running_mean: BUFFER target='features.8.conv.1.1.running_mean' persistent=True
b_features_8_conv_1_1_running_var: BUFFER target='features.8.conv.1.1.running_var' persistent=True
b_features_8_conv_1_1_num_batches_tracked: BUFFER target='features.8.conv.1.1.num_batches_tracked' persistent=True
b_features_8_conv_3_running_mean: BUFFER target='features.8.conv.3.running_mean' persistent=True
b_features_8_conv_3_running_var: BUFFER target='features.8.conv.3.running_var' persistent=True
b_features_8_conv_3_num_batches_tracked: BUFFER target='features.8.conv.3.num_batches_tracked' persistent=True
b_features_9_conv_0_1_running_mean: BUFFER target='features.9.conv.0.1.running_mean' persistent=True
b_features_9_conv_0_1_running_var: BUFFER target='features.9.conv.0.1.running_var' persistent=True
b_features_9_conv_0_1_num_batches_tracked: BUFFER target='features.9.conv.0.1.num_batches_tracked' persistent=True
b_features_9_conv_1_1_running_mean: BUFFER target='features.9.conv.1.1.running_mean' persistent=True
b_features_9_conv_1_1_running_var: BUFFER target='features.9.conv.1.1.running_var' persistent=True
b_features_9_conv_1_1_num_batches_tracked: BUFFER target='features.9.conv.1.1.num_batches_tracked' persistent=True
b_features_9_conv_3_running_mean: BUFFER target='features.9.conv.3.running_mean' persistent=True
b_features_9_conv_3_running_var: BUFFER target='features.9.conv.3.running_var' persistent=True
b_features_9_conv_3_num_batches_tracked: BUFFER target='features.9.conv.3.num_batches_tracked' persistent=True
b_features_10_conv_0_1_running_mean: BUFFER target='features.10.conv.0.1.running_mean' persistent=True
b_features_10_conv_0_1_running_var: BUFFER target='features.10.conv.0.1.running_var' persistent=True
b_features_10_conv_0_1_num_batches_tracked: BUFFER target='features.10.conv.0.1.num_batches_tracked' persistent=True
b_features_10_conv_1_1_running_mean: BUFFER target='features.10.conv.1.1.running_mean' persistent=True
b_features_10_conv_1_1_running_var: BUFFER target='features.10.conv.1.1.running_var' persistent=True
b_features_10_conv_1_1_num_batches_tracked: BUFFER target='features.10.conv.1.1.num_batches_tracked' persistent=True
b_features_10_conv_3_running_mean: BUFFER target='features.10.conv.3.running_mean' persistent=True
b_features_10_conv_3_running_var: BUFFER target='features.10.conv.3.running_var' persistent=True
b_features_10_conv_3_num_batches_tracked: BUFFER target='features.10.conv.3.num_batches_tracked' persistent=True
b_features_11_conv_0_1_running_mean: BUFFER target='features.11.conv.0.1.running_mean' persistent=True
b_features_11_conv_0_1_running_var: BUFFER target='features.11.conv.0.1.running_var' persistent=True
b_features_11_conv_0_1_num_batches_tracked: BUFFER target='features.11.conv.0.1.num_batches_tracked' persistent=True
b_features_11_conv_1_1_running_mean: BUFFER target='features.11.conv.1.1.running_mean' persistent=True
b_features_11_conv_1_1_running_var: BUFFER target='features.11.conv.1.1.running_var' persistent=True
b_features_11_conv_1_1_num_batches_tracked: BUFFER target='features.11.conv.1.1.num_batches_tracked' persistent=True
b_features_11_conv_3_running_mean: BUFFER target='features.11.conv.3.running_mean' persistent=True
b_features_11_conv_3_running_var: BUFFER target='features.11.conv.3.running_var' persistent=True
b_features_11_conv_3_num_batches_tracked: BUFFER target='features.11.conv.3.num_batches_tracked' persistent=True
b_features_12_conv_0_1_running_mean: BUFFER target='features.12.conv.0.1.running_mean' persistent=True
b_features_12_conv_0_1_running_var: BUFFER target='features.12.conv.0.1.running_var' persistent=True
b_features_12_conv_0_1_num_batches_tracked: BUFFER target='features.12.conv.0.1.num_batches_tracked' persistent=True
b_features_12_conv_1_1_running_mean: BUFFER target='features.12.conv.1.1.running_mean' persistent=True
b_features_12_conv_1_1_running_var: BUFFER target='features.12.conv.1.1.running_var' persistent=True
b_features_12_conv_1_1_num_batches_tracked: BUFFER target='features.12.conv.1.1.num_batches_tracked' persistent=True
b_features_12_conv_3_running_mean: BUFFER target='features.12.conv.3.running_mean' persistent=True
b_features_12_conv_3_running_var: BUFFER target='features.12.conv.3.running_var' persistent=True
b_features_12_conv_3_num_batches_tracked: BUFFER target='features.12.conv.3.num_batches_tracked' persistent=True
b_features_13_conv_0_1_running_mean: BUFFER target='features.13.conv.0.1.running_mean' persistent=True
b_features_13_conv_0_1_running_var: BUFFER target='features.13.conv.0.1.running_var' persistent=True
b_features_13_conv_0_1_num_batches_tracked: BUFFER target='features.13.conv.0.1.num_batches_tracked' persistent=True
b_features_13_conv_1_1_running_mean: BUFFER target='features.13.conv.1.1.running_mean' persistent=True
b_features_13_conv_1_1_running_var: BUFFER target='features.13.conv.1.1.running_var' persistent=True
b_features_13_conv_1_1_num_batches_tracked: BUFFER target='features.13.conv.1.1.num_batches_tracked' persistent=True
b_features_13_conv_3_running_mean: BUFFER target='features.13.conv.3.running_mean' persistent=True
b_features_13_conv_3_running_var: BUFFER target='features.13.conv.3.running_var' persistent=True
b_features_13_conv_3_num_batches_tracked: BUFFER target='features.13.conv.3.num_batches_tracked' persistent=True
b_features_14_conv_0_1_running_mean: BUFFER target='features.14.conv.0.1.running_mean' persistent=True
b_features_14_conv_0_1_running_var: BUFFER target='features.14.conv.0.1.running_var' persistent=True
b_features_14_conv_0_1_num_batches_tracked: BUFFER target='features.14.conv.0.1.num_batches_tracked' persistent=True
b_features_14_conv_1_1_running_mean: BUFFER target='features.14.conv.1.1.running_mean' persistent=True
b_features_14_conv_1_1_running_var: BUFFER target='features.14.conv.1.1.running_var' persistent=True
b_features_14_conv_1_1_num_batches_tracked: BUFFER target='features.14.conv.1.1.num_batches_tracked' persistent=True
b_features_14_conv_3_running_mean: BUFFER target='features.14.conv.3.running_mean' persistent=True
b_features_14_conv_3_running_var: BUFFER target='features.14.conv.3.running_var' persistent=True
b_features_14_conv_3_num_batches_tracked: BUFFER target='features.14.conv.3.num_batches_tracked' persistent=True
b_features_15_conv_0_1_running_mean: BUFFER target='features.15.conv.0.1.running_mean' persistent=True
b_features_15_conv_0_1_running_var: BUFFER target='features.15.conv.0.1.running_var' persistent=True
b_features_15_conv_0_1_num_batches_tracked: BUFFER target='features.15.conv.0.1.num_batches_tracked' persistent=True
b_features_15_conv_1_1_running_mean: BUFFER target='features.15.conv.1.1.running_mean' persistent=True
b_features_15_conv_1_1_running_var: BUFFER target='features.15.conv.1.1.running_var' persistent=True
b_features_15_conv_1_1_num_batches_tracked: BUFFER target='features.15.conv.1.1.num_batches_tracked' persistent=True
b_features_15_conv_3_running_mean: BUFFER target='features.15.conv.3.running_mean' persistent=True
b_features_15_conv_3_running_var: BUFFER target='features.15.conv.3.running_var' persistent=True
b_features_15_conv_3_num_batches_tracked: BUFFER target='features.15.conv.3.num_batches_tracked' persistent=True
b_features_16_conv_0_1_running_mean: BUFFER target='features.16.conv.0.1.running_mean' persistent=True
b_features_16_conv_0_1_running_var: BUFFER target='features.16.conv.0.1.running_var' persistent=True
b_features_16_conv_0_1_num_batches_tracked: BUFFER target='features.16.conv.0.1.num_batches_tracked' persistent=True
b_features_16_conv_1_1_running_mean: BUFFER target='features.16.conv.1.1.running_mean' persistent=True
b_features_16_conv_1_1_running_var: BUFFER target='features.16.conv.1.1.running_var' persistent=True
b_features_16_conv_1_1_num_batches_tracked: BUFFER target='features.16.conv.1.1.num_batches_tracked' persistent=True
b_features_16_conv_3_running_mean: BUFFER target='features.16.conv.3.running_mean' persistent=True
b_features_16_conv_3_running_var: BUFFER target='features.16.conv.3.running_var' persistent=True
b_features_16_conv_3_num_batches_tracked: BUFFER target='features.16.conv.3.num_batches_tracked' persistent=True
b_features_17_conv_0_1_running_mean: BUFFER target='features.17.conv.0.1.running_mean' persistent=True
b_features_17_conv_0_1_running_var: BUFFER target='features.17.conv.0.1.running_var' persistent=True
b_features_17_conv_0_1_num_batches_tracked: BUFFER target='features.17.conv.0.1.num_batches_tracked' persistent=True
b_features_17_conv_1_1_running_mean: BUFFER target='features.17.conv.1.1.running_mean' persistent=True
b_features_17_conv_1_1_running_var: BUFFER target='features.17.conv.1.1.running_var' persistent=True
b_features_17_conv_1_1_num_batches_tracked: BUFFER target='features.17.conv.1.1.num_batches_tracked' persistent=True
b_features_17_conv_3_running_mean: BUFFER target='features.17.conv.3.running_mean' persistent=True
b_features_17_conv_3_running_var: BUFFER target='features.17.conv.3.running_var' persistent=True
b_features_17_conv_3_num_batches_tracked: BUFFER target='features.17.conv.3.num_batches_tracked' persistent=True
b_features_18_1_running_mean: BUFFER target='features.18.1.running_mean' persistent=True
b_features_18_1_running_var: BUFFER target='features.18.1.running_var' persistent=True
b_features_18_1_num_batches_tracked: BUFFER target='features.18.1.num_batches_tracked' persistent=True
x: USER_INPUT

# outputs
linear: USER_OUTPUT
Ç8
2pkg.torch.export.ExportedProgram.range_constraints{}B
 